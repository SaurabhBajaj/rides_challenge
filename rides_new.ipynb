{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import geohash\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # references\n",
    "# # https://github.com/nlintz/TensorFlow-Tutorials\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py    \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/estimators/abalone.py\n",
    "# https://github.com/lyft/challenge/blob/master/data_scientist/travel_time/description/travel_time.pdf\n",
    "# linear regression https://aqibsaeed.github.io/2016-07-07-TensorflowLR/\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# raw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n",
    "#         'last_name': ['Miller', 'Jacobson', \".\", 'Milner', 'Cooze'],\n",
    "#         'age': [42, 52, 36, 24, 73],\n",
    "#         'preTestScore': [4, 24, 31, \".\", \".\"],\n",
    "#         'postTestScore': [\"25,000\", \"94,000\", 57, 62, 70]}\n",
    "# df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore'])\n",
    "# df\n",
    "inputdf = pd.read_csv(\"train.csv\")\n",
    "df = inputdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_training_data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>start_geohash</th>\n",
       "      <th>end_geohash</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dr5rsjx</td>\n",
       "      <td>dr5rspd</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>dr5rsjx:dr5rspd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dr5rusx</td>\n",
       "      <td>dr5ruy5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rusx:dr5ruy5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dr5rvjh</td>\n",
       "      <td>dr5rvnm</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rvjh:dr5rvnm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dr5ru2k</td>\n",
       "      <td>dr72jhr</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>dr5ru2k:dr72jhr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dr5rug9</td>\n",
       "      <td>dr5ruqp</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>dr5rug9:dr5ruqp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dr5ru2k</td>\n",
       "      <td>dr5rst4</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>dr5ru2k:dr5rst4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dr5ruuv</td>\n",
       "      <td>dr5ru0v</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>dr5ruuv:dr5ru0v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dr5ru14</td>\n",
       "      <td>dr5ru53</td>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>dr5ru14:dr5ru53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dr5rvj5</td>\n",
       "      <td>dr5ru6d</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>dr5rvj5:dr5ru6d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dr5rus3</td>\n",
       "      <td>dr5ruv0</td>\n",
       "      <td>13</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>dr5rus3:dr5ruv0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id start_geohash end_geohash  hour  minute  weekday              geo\n",
       "0       0       dr5rsjx     dr5rspd     3      13        6  dr5rsjx:dr5rspd\n",
       "1       1       dr5rusx     dr5ruy5    13       2        1  dr5rusx:dr5ruy5\n",
       "2       2       dr5rvjh     dr5rvnm    13       2        1  dr5rvjh:dr5rvnm\n",
       "3       3       dr5ru2k     dr72jhr     4       8        3  dr5ru2k:dr72jhr\n",
       "4       4       dr5rug9     dr5ruqp    23       9        2  dr5rug9:dr5ruqp\n",
       "5       5       dr5ru2k     dr5rst4    20      41        3  dr5ru2k:dr5rst4\n",
       "6       6       dr5ruuv     dr5ru0v    22      37        5  dr5ruuv:dr5ru0v\n",
       "7       7       dr5ru14     dr5ru53    11      58        5  dr5ru14:dr5ru53\n",
       "8       8       dr5rvj5     dr5ru6d    23      22        2  dr5rvj5:dr5ru6d\n",
       "9       9       dr5rus3     dr5ruv0    13      56        3  dr5rus3:dr5ruv0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "\n",
    "def transform_dataset(df):\n",
    "    def get_geohash_from_concat_latlon(latlon):\n",
    "        lat, lon = latlon.split(\":\")\n",
    "        lat = float(lat)\n",
    "        lon = float(lon)        \n",
    "        precision = 7    \n",
    "        if (lat < -1000.0 or lat > 1000.0 or lon < -1000.0 or lon > 1000.0):\n",
    "            return 0\n",
    "        return geohash.encode(lat, lon, precision)\n",
    "\n",
    "    tz = pytz.timezone('America/New_York')\n",
    "    def get_local_time(timestamp):\n",
    "        d = datetime.utcfromtimestamp(timestamp).replace(tzinfo=pytz.utc)\n",
    "        dt = d.astimezone(tz)\n",
    "        return dt\n",
    "\n",
    "    def get_start_of_the_day(start_timestamp):\n",
    "        return start_timestamp.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    \n",
    "#     d[\"geo\"] = d[\"start_geohash\"] + \":\" + d[\"end_geohash\"]\n",
    "    start_geo_1 = df[\"start_lat\"].astype(str) + \":\" + df[\"start_lng\"].astype(str)\n",
    "    start_geo = start_geo_1.apply(get_geohash_from_concat_latlon)\n",
    "    start_geo.name = 'start_geohash'\n",
    "    df = pd.concat([df, start_geo], axis = 1)    \n",
    "    del start_geo_1\n",
    "\n",
    "    end_geo_1 = df[\"end_lat\"].astype(str) + \":\" + df[\"end_lng\"].astype(str)\n",
    "    end_geo = end_geo_1.apply(get_geohash_from_concat_latlon)\n",
    "    end_geo.name = 'end_geohash'\n",
    "    df = pd.concat([df, end_geo], axis = 1)    \n",
    "    del end_geo_1\n",
    "    \n",
    "    start_time = df[\"start_timestamp\"].apply(get_local_time)\n",
    "    start_time.name = 'start_timestamp_obj'\n",
    "    df = pd.concat([df, start_time], axis = 1)    \n",
    "    # start_time[:10]\n",
    "\n",
    "    start = df[\"start_timestamp_obj\"].apply(get_start_of_the_day)\n",
    "    start.name = 'start_of_day'\n",
    "    df = pd.concat([df, start], axis = 1)\n",
    "\n",
    "    hour = df[\"start_timestamp_obj\"].dt.hour\n",
    "    hour.name = 'hour'\n",
    "    df = pd.concat([df, hour], axis = 1)\n",
    "\n",
    "    minute = df[\"start_timestamp_obj\"].dt.minute\n",
    "    minute.name = 'minute'\n",
    "    df = pd.concat([df, minute], axis = 1)\n",
    "\n",
    "    weekday = df[\"start_timestamp_obj\"].dt.weekday\n",
    "    weekday.name = 'weekday'\n",
    "    df = pd.concat([df, weekday], axis = 1)\n",
    "\n",
    "    df[\"geo\"] = df[\"start_geohash\"] + \":\" + df[\"end_geohash\"]\n",
    "    del start_time\n",
    "    del end_geo\n",
    "    del hour\n",
    "    del minute\n",
    "    del weekday\n",
    "    outdf = df.copy()\n",
    "    outdf.drop('start_lng', axis=1, inplace=True)\n",
    "    outdf.drop('start_lat', axis=1, inplace=True)\n",
    "    outdf.drop('end_lng', axis=1, inplace=True)\n",
    "    outdf.drop('end_lat', axis=1, inplace=True)\n",
    "    outdf.drop('start_timestamp', axis=1, inplace=True)\n",
    "    outdf.drop('start_timestamp_obj', axis=1, inplace=True)\n",
    "    outdf.drop('start_of_day', axis=1, inplace=True)\n",
    "    return outdf\n",
    "#     df = pd.concat([df, pd.DataFrame(start_time, index=df.index)], axis = 1)\n",
    "dfff = pd.read_csv(\"test.csv\")\n",
    "d = dfff[:100].copy()\n",
    "d = transform_dataset(d)\n",
    "d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = transform_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_geohash</th>\n",
       "      <th>end_geohash</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>dr5regs</td>\n",
       "      <td>dr5reuz</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>dr5regs:dr5reuz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  duration start_geohash end_geohash  hour  minute  weekday  \\\n",
       "0       0       112       dr5regs     dr5reuz    23      33        5   \n",
       "\n",
       "               geo  \n",
       "0  dr5regs:dr5reuz  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_csv(\"train_mod.csv\", index=False)\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# g = set()\n",
    "# dictionary = {}\n",
    "# reverse_dictionary = {}\n",
    "# for index, row in df.iterrows():\n",
    "#     g.add(row['start_geohash'])\n",
    "#     g.add(row['end_geohash'])\n",
    "# for i, item in enumerate(g):\n",
    "#     dictionary[i] = item\n",
    "#     reverse_dictionary[item] = i\n",
    "\n",
    "# tup = defaultdict(list)\n",
    "# for index, row in df.iterrows():\n",
    "#     tup[row['start_geohash']].append((row['duration'], row['end_geohash']))\n",
    "# glist = list(g)\n",
    "    \n",
    "# #     gmap[row['start_geohash']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary = {}\n",
    "reverse_dictionary = {}\n",
    "start_list = list(df['start_geohash'])\n",
    "end_list = list(df['end_geohash'])\n",
    "geoset = set(start_list + end_list)\n",
    "\n",
    "for i, item in enumerate(geoset):\n",
    "    dictionary[i] = item\n",
    "    reverse_dictionary[item] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tup = defaultdict(list)\n",
    "def geo_mapping(combo):\n",
    "    start, end, duration = combo.split(\":\")\n",
    "    tup[start].append((int(duration), end))\n",
    "    \n",
    "# df[\"geo\"] = df[\"start_geohash\"] + \":\" + df[\"end_geohash\"] + \":\" + df[\"duration\"].astype(str)\n",
    "_ = df[\"geo\"].apply(geo_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted_tup = {}\n",
    "for k, v in tup.iteritems():\n",
    "    sorted_tup[k] = sorted(tup[k])[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dict_head(d, num=10):\n",
    "    i = 0\n",
    "    for k, v in d.iteritems():\n",
    "        print k, v\n",
    "        i += 1\n",
    "        if i > num:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr5x2fk 43101\n",
      "dr727j3 22636\n",
      "dr724d9 1\n",
      "dr724d3 3\n",
      "dr724d7 4\n",
      "dr5x9zz 5\n",
      "dr5x9zy 6\n",
      "dr5x9zw 7\n",
      "dr5x9zv 8\n",
      "dr5x9zu 9\n",
      "dr5x9zt 10\n"
     ]
    }
   ],
   "source": [
    "# dict_head(reverse_dictionary)\n",
    "dict_head(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch [ 0  2  4  5  6  9 15 21 23 34]\n",
      "labels [[    0]\n",
      " [ 7645]\n",
      " [    4]\n",
      " [15182]\n",
      " [20409]\n",
      " [39681]\n",
      " [   15]\n",
      " [ 6454]\n",
      " [23540]\n",
      " [ 8020]]\n"
     ]
    }
   ],
   "source": [
    "### BEGINNING OF THE EMBEDDINGS MODEL\n",
    "\n",
    "def generate_batch(batch_size):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    dictionary[data_index]\n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        item = data_index\n",
    "        element = sorted_tup.get(dictionary[data_index])\n",
    "        if not element:\n",
    "            data_index = (data_index + 1) % len(sorted_tup)\n",
    "            continue\n",
    "        options = sorted_tup[dictionary[data_index]]\n",
    "        choice = random.choice(options[:300])\n",
    "        batch[i] = item\n",
    "        labels[i, 0] = reverse_dictionary[choice[1]]\n",
    "        data_index = (data_index + 1) % len(sorted_tup)\n",
    "        i += 1\n",
    "    return batch, labels\n",
    "batch, labels = generate_batch(10)\n",
    "print 'batch', batch\n",
    "print 'labels', labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "embedding_size = 8  # Dimension of the embedding vector.\n",
    "num_sampled = 256\n",
    "vocabulary_size = len(geoset)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "#     valid_embeddings = tf.nn.embedding_lookup(\n",
    "#       normalized_embeddings, valid_dataset)\n",
    "#     similarity = tf.matmul(\n",
    "#       valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 922.055908203125)\n",
      "('Average loss at step ', 2000, ': ', 255.02827359390258)\n",
      "('Average loss at step ', 4000, ': ', 22.521580685138701)\n",
      "('Average loss at step ', 6000, ': ', 6.9114262388944629)\n",
      "('Average loss at step ', 8000, ': ', 4.6309022707939151)\n",
      "('Average loss at step ', 10000, ': ', 3.9552785657644272)\n",
      "('Average loss at step ', 12000, ': ', 3.6165919969081877)\n",
      "('Average loss at step ', 14000, ': ', 3.3777582536935808)\n",
      "('Average loss at step ', 16000, ': ', 3.1402888902425765)\n",
      "('Average loss at step ', 18000, ': ', 2.9560081437826158)\n",
      "('Average loss at step ', 20000, ': ', 2.7861582313776014)\n",
      "('Average loss at step ', 22000, ': ', 2.6414496734738351)\n",
      "('Average loss at step ', 24000, ': ', 2.5255686650276186)\n",
      "('Average loss at step ', 26000, ': ', 2.4219257459640504)\n",
      "('Average loss at step ', 28000, ': ', 2.3250955010056495)\n",
      "('Average loss at step ', 30000, ': ', 2.2503695976138114)\n",
      "('Average loss at step ', 32000, ': ', 2.1697036465406416)\n",
      "('Average loss at step ', 34000, ': ', 2.106338211417198)\n",
      "('Average loss at step ', 36000, ': ', 2.0463817286491395)\n",
      "('Average loss at step ', 38000, ': ', 2.0015400566458701)\n",
      "('Average loss at step ', 40000, ': ', 1.9422867895364762)\n",
      "('Average loss at step ', 42000, ': ', 1.9040037644505501)\n",
      "('Average loss at step ', 44000, ': ', 1.8611768037080765)\n",
      "('Average loss at step ', 46000, ': ', 1.8087452000975608)\n",
      "('Average loss at step ', 48000, ': ', 1.7809075533747674)\n",
      "('Average loss at step ', 50000, ': ', 1.750354264497757)\n",
      "('Average loss at step ', 52000, ': ', 1.7145462095141411)\n",
      "('Average loss at step ', 54000, ': ', 1.6904569826722144)\n",
      "('Average loss at step ', 56000, ': ', 1.6515648226737976)\n",
      "('Average loss at step ', 58000, ': ', 1.6314799477458)\n",
      "('Average loss at step ', 60000, ': ', 1.6039730417728424)\n",
      "('Average loss at step ', 62000, ': ', 1.5732253646850587)\n",
      "('Average loss at step ', 64000, ': ', 1.5600167859196663)\n",
      "('Average loss at step ', 66000, ': ', 1.5265585419535637)\n",
      "('Average loss at step ', 68000, ': ', 1.5110904302001)\n",
      "('Average loss at step ', 70000, ': ', 1.4847918127775193)\n",
      "('Average loss at step ', 72000, ': ', 1.4609172398149968)\n",
      "('Average loss at step ', 74000, ': ', 1.4510642374157905)\n",
      "('Average loss at step ', 76000, ': ', 1.4322495059967042)\n",
      "('Average loss at step ', 78000, ': ', 1.4055045038461684)\n",
      "('Average loss at step ', 80000, ': ', 1.3900282690227033)\n",
      "('Average loss at step ', 82000, ': ', 1.3733818891942502)\n",
      "('Average loss at step ', 84000, ': ', 1.3584368508756162)\n",
      "('Average loss at step ', 86000, ': ', 1.341959097802639)\n",
      "('Average loss at step ', 88000, ': ', 1.3274171043634415)\n",
      "('Average loss at step ', 90000, ': ', 1.3157512641251088)\n",
      "('Average loss at step ', 92000, ': ', 1.2956108925938605)\n",
      "('Average loss at step ', 94000, ': ', 1.282130063086748)\n",
      "('Average loss at step ', 96000, ': ', 1.2710861873626709)\n",
      "('Average loss at step ', 98000, ': ', 1.2541229363083839)\n",
      "('Average loss at step ', 100000, ': ', 1.2447166712880136)\n",
      "('Average loss at step ', 102000, ': ', 1.2295111821293832)\n",
      "('Average loss at step ', 104000, ': ', 1.2196628922224044)\n",
      "('Average loss at step ', 106000, ': ', 1.2077539195418359)\n",
      "('Average loss at step ', 108000, ': ', 1.1876350183486939)\n",
      "('Average loss at step ', 110000, ': ', 1.1779068050682544)\n",
      "('Average loss at step ', 112000, ': ', 1.1750560702979564)\n",
      "('Average loss at step ', 114000, ': ', 1.156787085711956)\n",
      "('Average loss at step ', 116000, ': ', 1.1533434065878392)\n",
      "('Average loss at step ', 118000, ': ', 1.1450811084210872)\n",
      "('Average loss at step ', 120000, ': ', 1.1322126869261264)\n",
      "('Average loss at step ', 122000, ': ', 1.1260487741231919)\n",
      "('Average loss at step ', 124000, ': ', 1.1139850745797157)\n",
      "('Average loss at step ', 126000, ': ', 1.1079882442057132)\n",
      "('Average loss at step ', 128000, ': ', 1.0886536381542682)\n",
      "('Average loss at step ', 130000, ': ', 1.0885968551039695)\n",
      "('Average loss at step ', 132000, ': ', 1.0711903384327888)\n",
      "('Average loss at step ', 134000, ': ', 1.0666506870388985)\n",
      "('Average loss at step ', 136000, ': ', 1.0594425651431083)\n",
      "('Average loss at step ', 138000, ': ', 1.0523907069563865)\n",
      "('Average loss at step ', 140000, ': ', 1.0424764206111432)\n",
      "('Average loss at step ', 142000, ': ', 1.0383950422406196)\n",
      "('Average loss at step ', 144000, ': ', 1.0254451321661473)\n",
      "('Average loss at step ', 146000, ': ', 1.023071388244629)\n",
      "('Average loss at step ', 148000, ': ', 1.0125379218757153)\n",
      "('Average loss at step ', 150000, ': ', 1.0031650956869125)\n",
      "('Average loss at step ', 152000, ': ', 1.0001103236079216)\n",
      "('Average loss at step ', 154000, ': ', 0.98803067106008524)\n",
      "('Average loss at step ', 156000, ': ', 0.98871384248137473)\n",
      "('Average loss at step ', 158000, ': ', 0.98194293668866162)\n",
      "('Average loss at step ', 160000, ': ', 0.97521558624506)\n",
      "('Average loss at step ', 162000, ': ', 0.96395196965336805)\n",
      "('Average loss at step ', 164000, ': ', 0.96195160791277889)\n",
      "('Average loss at step ', 166000, ': ', 0.9513096430599689)\n",
      "('Average loss at step ', 168000, ': ', 0.94600727319717404)\n",
      "('Average loss at step ', 170000, ': ', 0.94539119975268837)\n",
      "('Average loss at step ', 172000, ': ', 0.93845876151323315)\n",
      "('Average loss at step ', 174000, ': ', 0.93156540304422375)\n",
      "('Average loss at step ', 176000, ': ', 0.92193444445729256)\n",
      "('Average loss at step ', 178000, ': ', 0.92390105313062665)\n",
      "('Average loss at step ', 180000, ': ', 0.91111016929149624)\n",
      "('Average loss at step ', 182000, ': ', 0.90877866473793989)\n",
      "('Average loss at step ', 184000, ': ', 0.90175113716721533)\n",
      "('Average loss at step ', 186000, ': ', 0.89971061563491816)\n",
      "('Average loss at step ', 188000, ': ', 0.89207469516992566)\n",
      "('Average loss at step ', 190000, ': ', 0.89113974764943127)\n",
      "('Average loss at step ', 192000, ': ', 0.8860537773966789)\n",
      "('Average loss at step ', 194000, ': ', 0.88111151096224782)\n",
      "('Average loss at step ', 196000, ': ', 0.87894158717989923)\n",
      "('Average loss at step ', 198000, ': ', 0.87060521121323109)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 200000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_batch = 32\n",
    "data_size = vocabulary_size\n",
    "training_size = vocabulary_size - vocabulary_size//10\n",
    "testing_size = data_size - training_size\n",
    "INPUT_FEATURES = 19\n",
    "# start_timestamp\tduration\tstart_geohash\tend_geohash\tstart_timestamp_obj\tstart_of_day\tweekday\thour\tminute\n",
    "\"\"\" Generate the input feature dataset \"\"\"\n",
    "def get_batch(batch_size, df):\n",
    "    global batch_index\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        index = (batch_index + i) % training_size\n",
    "        batch[i, 0] = df.iloc[index][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[index][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[index][\"minute\"]\n",
    "        start_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[index][\"start_geohash\"]]]\n",
    "        end_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[index][\"end_geohash\"]]]\n",
    "        embedding_start_index = 3\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + embedding_start_index] = start_geohash_embedding[j]\n",
    "        end_geohash_emebedding_start = embedding_start_index + embedding_size\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + end_geohash_emebedding_start] = start_geohash_embedding[j]\n",
    "        labels[i, 0] = df.iloc[index][\"duration\"]\n",
    "    batch_index += batch_size % training_size\n",
    "    return batch, labels\n",
    "\n",
    "def get_test_batch(batch_size, df):\n",
    "    indices = []\n",
    "    for i in range(batch_size):\n",
    "        indices.append(random.randrange(0, len(df)))\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "    for i, position in enumerate(indices):\n",
    "        batch[i, 0] = df.iloc[position][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[position][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[position][\"minute\"]\n",
    "        start_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[position][\"start_geohash\"]]]\n",
    "        end_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[position][\"end_geohash\"]]]\n",
    "        embedding_start_index = 3\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + embedding_start_index] = start_geohash_embedding[j]\n",
    "        end_geohash_emebedding_start = embedding_start_index + embedding_size\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + end_geohash_emebedding_start] = start_geohash_embedding[j]\n",
    "        labels[i, 0] = df.iloc[position][\"duration\"]\n",
    "    return batch, labels\n",
    "\n",
    "def get_full_test_data(limit=None):\n",
    "    path = \"test.csv\"\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    if limit:\n",
    "        df = df[:limit]\n",
    "    df = transform_dataset(df)\n",
    "    batch_size = len(df)\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        batch[i, 0] = df.iloc[i][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[i][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[i][\"minute\"]\n",
    "        start_geohash_index = reverse_dictionary.get(df.iloc[i][\"start_geohash\"], random.randrange(0, 1000))\n",
    "        end_geohash_index = reverse_dictionary.get(df.iloc[i][\"end_geohash\"], random.randrange(0, 1000))\n",
    "        start_geohash_embedding = final_embeddings[start_geohash_index]\n",
    "        end_geohash_embedding = final_embeddings[end_geohash_index]\n",
    "        embedding_start_index = 3\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + embedding_start_index] = start_geohash_embedding[j]\n",
    "        end_geohash_emebedding_start = embedding_start_index + embedding_size\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + end_geohash_emebedding_start] = start_geohash_embedding[j]\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_placeholders():\n",
    "    with tf.name_scope('input_layer'):\n",
    "        features = tf.placeholder(tf.float32, shape=[None, INPUT_FEATURES])\n",
    "        train_labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    return features, train_labels\n",
    "def inference(features, hidden1_units, hidden2_units):\n",
    "    \"\"\" Building the tensorflow model \"\"\"\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights_1 = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_FEATURES, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_FEATURES))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features, weights_1) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights_2 = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights_2) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, 1],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([1]),\n",
    "                             name='biases')\n",
    "        output_linear = tf.matmul(hidden2, weights) + biases\n",
    "    return output_linear, (weights, weights_1, weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_loss(output_linear, labels, n_samples):\n",
    "#     \"\"\"Calculates the loss from the output_linear and the labels.\n",
    "#     Args:\n",
    "#       logits: output_linear tensor, float - [batch_size, 1].\n",
    "#       labels: Labels tensor, int32 - [batch_size].\n",
    "#     Returns:\n",
    "#       loss: Loss tensor of type float.\n",
    "#     \"\"\"\n",
    "#     labels = tf.to_float(labels)\n",
    "# #     cost = tf.reduce_mean(tf.square((output_linear - labels)/labels))\n",
    "#     cost = tf.reduce_sum(tf.pow(output_linear-labels, 2))/(2*n_samples)\n",
    "#     return cost\n",
    "\n",
    "def get_loss(output_linear, labels, n_samples):\n",
    "    \"\"\"Calculates the loss from the output_linear and the labels.\n",
    "    Args:\n",
    "      logits: output_linear tensor, float - [batch_size, 1].\n",
    "      labels: Labels tensor, int32 - [batch_size].\n",
    "    Returns:\n",
    "      loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "#     cost = tf.reduce_mean(tf.pow(output_linear-labels, 2))/(2*n_samples)\n",
    "    cost = tf.reduce_sum(tf.square(output_linear-labels))/(2*n_samples)\n",
    "#     reduction = tf.square(tf.subtract(output_linear, labels))\n",
    "#     red = tf.reduce_mean(reduction)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def training(loss, starter_learning_rate, decay_steps):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "    Args:\n",
    "      loss: Loss tensor, from loss().\n",
    "      learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               decay_steps, 0.96, staircase=True)   \n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_evaluation(output_linear, labels):\n",
    "\n",
    "    diff = tf.abs(tf.subtract(output_linear, labels))\n",
    "    return tf.reduce_mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full_test_data = batch_data\n",
    "full_test_data = get_full_test_data(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: avg loss = 0.01713 (0.018 sec)\n",
      "Mean difference in testing data : 735.491\n",
      "Step 2000: avg loss = 48.15249 (0.008 sec)\n",
      "Step 894000: avg loss = 43.59275 (0.009 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 896000: avg loss = 28.31786 (0.009 sec)\n",
      "Step 898000: avg loss = 39.54520 (0.009 sec)\n",
      "Step 900000: avg loss = 33.34905 (0.010 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 902000: avg loss = 28.56207 (0.007 sec)\n",
      "Step 904000: avg loss = 39.84820 (0.009 sec)\n",
      "Step 906000: avg loss = 32.14401 (0.010 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 908000: avg loss = 30.82403 (0.010 sec)\n",
      "Step 910000: avg loss = 41.90721 (0.007 sec)\n",
      "Step 912000: avg loss = 28.49724 (0.009 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 914000: avg loss = 39.68823 (0.007 sec)\n",
      "Step 916000: avg loss = 32.29090 (0.007 sec)\n",
      "Step 918000: avg loss = 30.89576 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 920000: avg loss = 42.38752 (0.007 sec)\n",
      "Step 922000: avg loss = 28.66122 (0.007 sec)\n",
      "Step 924000: avg loss = 39.10655 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 926000: avg loss = 33.11148 (0.007 sec)\n",
      "Step 928000: avg loss = 29.69036 (0.007 sec)\n",
      "Step 930000: avg loss = 43.19832 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 932000: avg loss = 28.31234 (0.008 sec)\n",
      "Step 934000: avg loss = 39.50454 (0.007 sec)\n",
      "Step 936000: avg loss = 33.14807 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 938000: avg loss = 28.76902 (0.007 sec)\n",
      "Step 940000: avg loss = 43.85309 (0.007 sec)\n",
      "Step 942000: avg loss = 28.48005 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 944000: avg loss = 39.29162 (0.007 sec)\n",
      "Step 946000: avg loss = 33.44763 (0.007 sec)\n",
      "Step 948000: avg loss = 28.75760 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 950000: avg loss = 43.59459 (0.007 sec)\n",
      "Step 952000: avg loss = 28.31096 (0.007 sec)\n",
      "Step 954000: avg loss = 39.51585 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 956000: avg loss = 33.36977 (0.007 sec)\n",
      "Step 958000: avg loss = 28.49736 (0.007 sec)\n",
      "Step 960000: avg loss = 39.92292 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 962000: avg loss = 32.14831 (0.008 sec)\n",
      "Step 964000: avg loss = 30.80475 (0.007 sec)\n",
      "Step 966000: avg loss = 41.93110 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 968000: avg loss = 28.48446 (0.007 sec)\n",
      "Step 970000: avg loss = 39.68264 (0.007 sec)\n",
      "Step 972000: avg loss = 32.30612 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 974000: avg loss = 30.89782 (0.007 sec)\n",
      "Step 976000: avg loss = 42.38101 (0.008 sec)\n",
      "Step 978000: avg loss = 28.65290 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 980000: avg loss = 39.12513 (0.007 sec)\n",
      "Step 982000: avg loss = 33.10841 (0.007 sec)\n",
      "Step 984000: avg loss = 29.62641 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 986000: avg loss = 43.22777 (0.007 sec)\n",
      "Step 988000: avg loss = 28.33481 (0.007 sec)\n",
      "Step 990000: avg loss = 39.45757 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 992000: avg loss = 33.14696 (0.007 sec)\n",
      "Step 994000: avg loss = 28.78977 (0.007 sec)\n",
      "Step 996000: avg loss = 43.88983 (0.007 sec)\n",
      "Mean difference in testing data : 391.078\n",
      "Step 998000: avg loss = 28.46702 (0.007 sec)\n"
     ]
    }
   ],
   "source": [
    "# data_size = vocabulary_size\n",
    "# training_size = vocabulary_size - vocabulary_size//10\n",
    "# testing_size = data_size - training_size\n",
    "final_results = None\n",
    "batch_index = 0\n",
    "num_steps = 1000000\n",
    "input_batch = 8\n",
    "starter_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "beta = 0.00001\n",
    "def start_training(num_steps):\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     tf.global_variables_initializer().run()\n",
    "    with tf.Graph().as_default():\n",
    "        features, labels = input_placeholders()\n",
    "        linear_inference, (w, w1, w2) = inference(features, 64, 16)\n",
    "        loss = get_loss(linear_inference, labels, training_size)\n",
    "        reg_loss = loss + beta*tf.nn.l2_loss(w1) + beta*tf.nn.l2_loss(w2)\n",
    "        train_op = training(reg_loss, starter_learning_rate, decay_steps)\n",
    "        evaluation = get_evaluation(linear_inference, labels)\n",
    "        summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/rides/\", sess.graph)\n",
    "        sess.run(init)\n",
    "        training_data = df[:training_size]\n",
    "        test_data = df[testing_size + 1:]\n",
    "        avg_loss = 0\n",
    "        validation_data, validation_labels = get_test_batch(10000, training_data)\n",
    "        for step in xrange(num_steps):\n",
    "            start_time = time.time()\n",
    "            batch_data, label_data = get_batch(input_batch, training_data)\n",
    "            feed_dict = {features:batch_data, labels:label_data}\n",
    "            _, loss_value = sess.run([train_op, reg_loss],\n",
    "                               feed_dict=feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            avg_loss += loss_value\n",
    "\n",
    "          # Write the summaries and print an overview fairly often.\n",
    "            if step % 2000 == 0:\n",
    "                # Print status to stdout.\n",
    "                print('Step %d: avg loss = %.5f (%.3f sec)' % (step, avg_loss/2000, duration))\n",
    "                avg_loss = 0\n",
    "                # Update the events file.\n",
    "#                 summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "#                 summary_writer.add_summary(summary_str, step)\n",
    "#                 summary_writer.flush()\n",
    "            if step % 6000 == 0:\n",
    "#                 test_batch_data, test_label_data = get_test_batch(5000, training_data)\n",
    "                feed_dict = {features:validation_data, labels:validation_labels}\n",
    "                results = sess.run(evaluation, feed_dict=feed_dict)\n",
    "                print 'Mean difference in testing data : %s' % results\n",
    "#         batch_data = get_full_test_data()\n",
    "        feed_dict = {features:full_test_data}\n",
    "        global final_results\n",
    "        final_results = sess.run(linear_inference, feed_dict=feed_dict)\n",
    "start_training(num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 740.24932861]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i in range(0, len(final_results))]\n",
    "final_output = pd.DataFrame({'row_id': index})\n",
    "final_output['duration'] = final_results[:,0]\n",
    "final_output['duration'] = final_output['duration'].astype(int)\n",
    "final_results[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_output.to_csv(\"duration.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/mchirico/bcc376fb336b73f24b29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def assert_geohashes_in_set(df, reverse_dictionary):\n",
    "    for index, row in df.iterrows():\n",
    "        missing = set()\n",
    "        if not reverse_dictionary.get(row[\"start_geohash\"]):\n",
    "            missing.add(row[\"start_geohash\"])\n",
    "        if not reverse_dictionary.get(row[\"end_geohash\"]):\n",
    "            missing.add(row[\"end_geohash\"])\n",
    "        if len(missing) > 0:\n",
    "            raise Exception(\"end_geohash is not in global dataset: %s\"% len(missing))\n",
    "\n",
    "def transform_input():\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    df['start_geohash'] = df.apply(geohash_encode_start, axis=1)\n",
    "    print 'end geohash'\n",
    "    df['end_geohash'] = df.apply(geohash_encode_end, axis=1)    \n",
    "#     df[\"start_timestamp_obj\"] = df.apply(get_local_time, axis=1)\n",
    "#     df.to_csv(\"test_mod.csv\")\n",
    "#     df[\"start_of_day\"] = df.apply(get_start_of_the_day, axis=1)\n",
    "#     df[\"weekday\"] = df.apply(get_weekday, axis=1)\n",
    "#     df[\"hour\"] = df.apply(get_hour, axis=1)\n",
    "#     df[\"minute\"] = df.apply(get_min, axis=1)\n",
    "    return df\n",
    "\n",
    "def handle_test_data():\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    df = transform_input(df)\n",
    "    assert_geohashes_in_set(df, reverse_dictionary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "end_geohash is not in global dataset: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-ed8a8f1e3703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dftest = transform_input()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# assert_geohashes_in_set(dftest, reverse_dictionary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0massert_geohashes_in_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-261-4adbaa7bf1e5>\u001b[0m in \u001b[0;36massert_geohashes_in_set\u001b[0;34m(df, reverse_dictionary)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_geohash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"end_geohash is not in global dataset: %s\"\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: end_geohash is not in global dataset: 1"
     ]
    }
   ],
   "source": [
    "# dftest = transform_input()\n",
    "# assert_geohashes_in_set(dftest, reverse_dictionary)\n",
    "assert_geohashes_in_set(dftest, reverse_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
