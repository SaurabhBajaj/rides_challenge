{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import geohash\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # references\n",
    "# # https://github.com/nlintz/TensorFlow-Tutorials\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py    \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/estimators/abalone.py\n",
    "# https://github.com/lyft/challenge/blob/master/data_scientist/travel_time/description/travel_time.pdf\n",
    "# linear regression https://aqibsaeed.github.io/2016-07-07-TensorflowLR/\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# raw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n",
    "#         'last_name': ['Miller', 'Jacobson', \".\", 'Milner', 'Cooze'],\n",
    "#         'age': [42, 52, 36, 24, 73],\n",
    "#         'preTestScore': [4, 24, 31, \".\", \".\"],\n",
    "#         'postTestScore': [\"25,000\", \"94,000\", 57, 62, 70]}\n",
    "# df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore'])\n",
    "# df\n",
    "inputdf = pd.read_csv(\"train.csv\")\n",
    "df = inputdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>start_geohash</th>\n",
       "      <th>end_geohash</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dr5rsjx</td>\n",
       "      <td>dr5rspd</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>dr5rsjx:dr5rspd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dr5rusx</td>\n",
       "      <td>dr5ruy5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rusx:dr5ruy5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dr5rvjh</td>\n",
       "      <td>dr5rvnm</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rvjh:dr5rvnm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dr5ru2k</td>\n",
       "      <td>dr72jhr</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>dr5ru2k:dr72jhr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dr5rug9</td>\n",
       "      <td>dr5ruqp</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>dr5rug9:dr5ruqp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dr5ru2k</td>\n",
       "      <td>dr5rst4</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>dr5ru2k:dr5rst4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dr5ruuv</td>\n",
       "      <td>dr5ru0v</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>dr5ruuv:dr5ru0v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dr5ru14</td>\n",
       "      <td>dr5ru53</td>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>dr5ru14:dr5ru53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dr5rvj5</td>\n",
       "      <td>dr5ru6d</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>dr5rvj5:dr5ru6d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dr5rus3</td>\n",
       "      <td>dr5ruv0</td>\n",
       "      <td>13</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>dr5rus3:dr5ruv0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id start_geohash end_geohash  hour  minute  weekday              geo\n",
       "0       0       dr5rsjx     dr5rspd     3      13        6  dr5rsjx:dr5rspd\n",
       "1       1       dr5rusx     dr5ruy5    13       2        1  dr5rusx:dr5ruy5\n",
       "2       2       dr5rvjh     dr5rvnm    13       2        1  dr5rvjh:dr5rvnm\n",
       "3       3       dr5ru2k     dr72jhr     4       8        3  dr5ru2k:dr72jhr\n",
       "4       4       dr5rug9     dr5ruqp    23       9        2  dr5rug9:dr5ruqp\n",
       "5       5       dr5ru2k     dr5rst4    20      41        3  dr5ru2k:dr5rst4\n",
       "6       6       dr5ruuv     dr5ru0v    22      37        5  dr5ruuv:dr5ru0v\n",
       "7       7       dr5ru14     dr5ru53    11      58        5  dr5ru14:dr5ru53\n",
       "8       8       dr5rvj5     dr5ru6d    23      22        2  dr5rvj5:dr5ru6d\n",
       "9       9       dr5rus3     dr5ruv0    13      56        3  dr5rus3:dr5ruv0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "\n",
    "def transform_dataset(df):\n",
    "    def get_geohash_from_concat_latlon(latlon):\n",
    "        lat, lon = latlon.split(\":\")\n",
    "        lat = float(lat)\n",
    "        lon = float(lon)        \n",
    "        precision = 7    \n",
    "        if (lat < -1000.0 or lat > 1000.0 or lon < -1000.0 or lon > 1000.0):\n",
    "            return 0\n",
    "        return geohash.encode(lat, lon, precision)\n",
    "\n",
    "    tz = pytz.timezone('America/New_York')\n",
    "    def get_local_time(timestamp):\n",
    "        d = datetime.utcfromtimestamp(timestamp).replace(tzinfo=pytz.utc)\n",
    "        dt = d.astimezone(tz)\n",
    "        return dt\n",
    "\n",
    "    def get_start_of_the_day(start_timestamp):\n",
    "        return start_timestamp.replace(hour=0, minute=0, second=0)\n",
    "\n",
    "    \n",
    "#     d[\"geo\"] = d[\"start_geohash\"] + \":\" + d[\"end_geohash\"]\n",
    "    start_geo_1 = df[\"start_lat\"].astype(str) + \":\" + df[\"start_lng\"].astype(str)\n",
    "    start_geo = start_geo_1.apply(get_geohash_from_concat_latlon)\n",
    "    start_geo.name = 'start_geohash'\n",
    "    df = pd.concat([df, start_geo], axis = 1)    \n",
    "    del start_geo_1\n",
    "\n",
    "    end_geo_1 = df[\"end_lat\"].astype(str) + \":\" + df[\"end_lng\"].astype(str)\n",
    "    end_geo = end_geo_1.apply(get_geohash_from_concat_latlon)\n",
    "    end_geo.name = 'end_geohash'\n",
    "    df = pd.concat([df, end_geo], axis = 1)    \n",
    "    del end_geo_1\n",
    "    \n",
    "    start_time = df[\"start_timestamp\"].apply(get_local_time)\n",
    "    start_time.name = 'start_timestamp_obj'\n",
    "    df = pd.concat([df, start_time], axis = 1)    \n",
    "    # start_time[:10]\n",
    "\n",
    "    start = df[\"start_timestamp_obj\"].apply(get_start_of_the_day)\n",
    "    start.name = 'start_of_day'\n",
    "    df = pd.concat([df, start], axis = 1)\n",
    "\n",
    "    hour = df[\"start_timestamp_obj\"].dt.hour\n",
    "    hour.name = 'hour'\n",
    "    df = pd.concat([df, hour], axis = 1)\n",
    "\n",
    "    minute = df[\"start_timestamp_obj\"].dt.minute\n",
    "    minute.name = 'minute'\n",
    "    df = pd.concat([df, minute], axis = 1)\n",
    "\n",
    "    weekday = df[\"start_timestamp_obj\"].dt.weekday\n",
    "    weekday.name = 'weekday'\n",
    "    df = pd.concat([df, weekday], axis = 1)\n",
    "\n",
    "    df[\"geo\"] = df[\"start_geohash\"] + \":\" + df[\"end_geohash\"]\n",
    "    del start_time\n",
    "    del end_geo\n",
    "    del hour\n",
    "    del minute\n",
    "    del weekday\n",
    "    outdf = df.copy()\n",
    "    outdf.drop('start_lng', axis=1, inplace=True)\n",
    "    outdf.drop('start_lat', axis=1, inplace=True)\n",
    "    outdf.drop('end_lng', axis=1, inplace=True)\n",
    "    outdf.drop('end_lat', axis=1, inplace=True)\n",
    "    outdf.drop('start_timestamp', axis=1, inplace=True)\n",
    "    outdf.drop('start_timestamp_obj', axis=1, inplace=True)\n",
    "    outdf.drop('start_of_day', axis=1, inplace=True)\n",
    "    return outdf\n",
    "#     df = pd.concat([df, pd.DataFrame(start_time, index=df.index)], axis = 1)\n",
    "dfff = pd.read_csv(\"test.csv\")\n",
    "d = dfff[:100].copy()\n",
    "d = transform_dataset(d)\n",
    "d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = transform_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_geohash</th>\n",
       "      <th>end_geohash</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>dr5regs</td>\n",
       "      <td>dr5reuz</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>dr5regs:dr5reuz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  duration start_geohash end_geohash  hour  minute  weekday  \\\n",
       "0       0       112       dr5regs     dr5reuz    23      33        5   \n",
       "\n",
       "               geo  \n",
       "0  dr5regs:dr5reuz  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_csv(\"train_mod.csv\", index=False)\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# g = set()\n",
    "# dictionary = {}\n",
    "# reverse_dictionary = {}\n",
    "# for index, row in df.iterrows():\n",
    "#     g.add(row['start_geohash'])\n",
    "#     g.add(row['end_geohash'])\n",
    "# for i, item in enumerate(g):\n",
    "#     dictionary[i] = item\n",
    "#     reverse_dictionary[item] = i\n",
    "\n",
    "# tup = defaultdict(list)\n",
    "# for index, row in df.iterrows():\n",
    "#     tup[row['start_geohash']].append((row['duration'], row['end_geohash']))\n",
    "# glist = list(g)\n",
    "    \n",
    "# #     gmap[row['start_geohash']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dictionary = {}\n",
    "reverse_dictionary = {}\n",
    "start_list = list(df['start_geohash'])\n",
    "end_list = list(df['end_geohash'])\n",
    "geoset = set(start_list + end_list)\n",
    "\n",
    "for i, item in enumerate(geoset):\n",
    "    dictionary[i] = item\n",
    "    reverse_dictionary[item] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tup = defaultdict(list)\n",
    "def geo_mapping(combo):\n",
    "    start, end, duration = combo.split(\":\")\n",
    "    tup[start].append((int(duration), end))\n",
    "    \n",
    "# df[\"geo\"] = df[\"start_geohash\"] + \":\" + df[\"end_geohash\"] + \":\" + df[\"duration\"].astype(str)\n",
    "_ = df[\"geo\"].apply(geo_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sorted_tup = {}\n",
    "for k, v in tup.iteritems():\n",
    "    sorted_tup[k] = sorted(tup[k])[:300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dict_head(d, num=10):\n",
    "    i = 0\n",
    "    for k, v in d.iteritems():\n",
    "        print k, v\n",
    "        i += 1\n",
    "        if i > num:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr5x2fk 43101\n",
      "dr727j3 22636\n",
      "dr724d9 1\n",
      "dr724d3 3\n",
      "dr724d7 4\n",
      "dr5x9zz 5\n",
      "dr5x9zy 6\n",
      "dr5x9zw 7\n",
      "dr5x9zv 8\n",
      "dr5x9zu 9\n",
      "dr5x9zt 10\n"
     ]
    }
   ],
   "source": [
    "# dict_head(reverse_dictionary)\n",
    "dict_head(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch [ 0  2  4  5  6  9 15 21 23 34]\n",
      "labels [[    0]\n",
      " [ 5133]\n",
      " [    4]\n",
      " [15182]\n",
      " [20409]\n",
      " [39681]\n",
      " [   15]\n",
      " [ 6454]\n",
      " [23540]\n",
      " [ 8020]]\n"
     ]
    }
   ],
   "source": [
    "### BEGINNING OF THE EMBEDDINGS MODEL\n",
    "\n",
    "def generate_batch(batch_size):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    dictionary[data_index]\n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        item = data_index\n",
    "        element = sorted_tup.get(dictionary[data_index])\n",
    "        if not element:\n",
    "            data_index = (data_index + 1) % len(sorted_tup)\n",
    "            continue\n",
    "        options = sorted_tup[dictionary[data_index]]\n",
    "        choice = random.choice(options[:300])\n",
    "        batch[i] = item\n",
    "        labels[i, 0] = reverse_dictionary[choice[1]]\n",
    "        data_index = (data_index + 1) % len(sorted_tup)\n",
    "        i += 1\n",
    "    return batch, labels\n",
    "batch, labels = generate_batch(10)\n",
    "print 'batch', batch\n",
    "print 'labels', labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embedding_size = 64  # Dimension of the embedding vector.\n",
    "num_sampled = 64\n",
    "vocabulary_size = len(geoset)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "#     valid_embeddings = tf.nn.embedding_lookup(\n",
    "#       normalized_embeddings, valid_dataset)\n",
    "#     similarity = tf.matmul(\n",
    "#       valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 276.7401123046875)\n",
      "('Average loss at step ', 2000, ': ', 152.06010083770752)\n",
      "('Average loss at step ', 4000, ': ', 72.915577672481533)\n",
      "('Average loss at step ', 6000, ': ', 33.382292408347126)\n",
      "('Average loss at step ', 8000, ': ', 18.646306135296822)\n",
      "('Average loss at step ', 10000, ': ', 12.102113711416722)\n",
      "('Average loss at step ', 12000, ': ', 7.9083063614815474)\n",
      "('Average loss at step ', 14000, ': ', 5.6802085246592764)\n",
      "('Average loss at step ', 16000, ': ', 3.8197205985933542)\n",
      "('Average loss at step ', 18000, ': ', 3.0549071153029801)\n",
      "('Average loss at step ', 20000, ': ', 2.4692361017763615)\n",
      "('Average loss at step ', 22000, ': ', 1.8839523471817374)\n",
      "('Average loss at step ', 24000, ': ', 1.5779076157510281)\n",
      "('Average loss at step ', 26000, ': ', 1.3261253466084599)\n",
      "('Average loss at step ', 28000, ': ', 1.1932069675326347)\n",
      "('Average loss at step ', 30000, ': ', 1.068209701269865)\n",
      "('Average loss at step ', 32000, ': ', 1.0161080236621201)\n",
      "('Average loss at step ', 34000, ': ', 0.94635198444873092)\n",
      "('Average loss at step ', 36000, ': ', 0.85903947379812595)\n",
      "('Average loss at step ', 38000, ': ', 0.83895285300910472)\n",
      "('Average loss at step ', 40000, ': ', 0.78875706013664604)\n",
      "('Average loss at step ', 42000, ': ', 0.76789344278350469)\n",
      "('Average loss at step ', 44000, ': ', 0.73224292431399229)\n",
      "('Average loss at step ', 46000, ': ', 0.71453321967646477)\n",
      "('Average loss at step ', 48000, ': ', 0.68245658641494811)\n",
      "('Average loss at step ', 50000, ': ', 0.65850403213500974)\n",
      "('Average loss at step ', 52000, ': ', 0.65560644463077189)\n",
      "('Average loss at step ', 54000, ': ', 0.63914972566440698)\n",
      "('Average loss at step ', 56000, ': ', 0.613542129335925)\n",
      "('Average loss at step ', 58000, ': ', 0.5937151236347854)\n",
      "('Average loss at step ', 60000, ': ', 0.56470724326558408)\n",
      "('Average loss at step ', 62000, ': ', 0.55934095964580777)\n",
      "('Average loss at step ', 64000, ': ', 0.5702527646888047)\n",
      "('Average loss at step ', 66000, ': ', 0.55083251717872916)\n",
      "('Average loss at step ', 68000, ': ', 0.52445277946256097)\n",
      "('Average loss at step ', 70000, ': ', 0.51311581030115483)\n",
      "('Average loss at step ', 72000, ': ', 0.5073489955477416)\n",
      "('Average loss at step ', 74000, ': ', 0.49963448791205883)\n",
      "('Average loss at step ', 76000, ': ', 0.4757557331752032)\n",
      "('Average loss at step ', 78000, ': ', 0.47813468546234072)\n",
      "('Average loss at step ', 80000, ': ', 0.46259378575347365)\n",
      "('Average loss at step ', 82000, ': ', 0.45819881695508957)\n",
      "('Average loss at step ', 84000, ': ', 0.44575612247735263)\n",
      "('Average loss at step ', 86000, ': ', 0.45105951179936526)\n",
      "('Average loss at step ', 88000, ': ', 0.42416491322033106)\n",
      "('Average loss at step ', 90000, ': ', 0.41703889712877573)\n",
      "('Average loss at step ', 92000, ': ', 0.41055074268206954)\n",
      "('Average loss at step ', 94000, ': ', 0.41338646481558683)\n",
      "('Average loss at step ', 96000, ': ', 0.40534736336395144)\n",
      "('Average loss at step ', 98000, ': ', 0.40250408343784511)\n",
      "('Average loss at step ', 100000, ': ', 0.38397538819257171)\n",
      "('Average loss at step ', 102000, ': ', 0.38413760780263695)\n",
      "('Average loss at step ', 104000, ': ', 0.37983631751872599)\n",
      "('Average loss at step ', 106000, ': ', 0.37783227045554668)\n",
      "('Average loss at step ', 108000, ': ', 0.36881753918156029)\n",
      "('Average loss at step ', 110000, ': ', 0.36690152402408421)\n",
      "('Average loss at step ', 112000, ': ', 0.35346230449900029)\n",
      "('Average loss at step ', 114000, ': ', 0.35488082706835122)\n",
      "('Average loss at step ', 116000, ': ', 0.35922251672670247)\n",
      "('Average loss at step ', 118000, ': ', 0.34397254218347373)\n",
      "('Average loss at step ', 120000, ': ', 0.33385887690819799)\n",
      "('Average loss at step ', 122000, ': ', 0.33100413494650277)\n",
      "('Average loss at step ', 124000, ': ', 0.33857245365064592)\n",
      "('Average loss at step ', 126000, ': ', 0.32616558890417219)\n",
      "('Average loss at step ', 128000, ': ', 0.32976808298192917)\n",
      "('Average loss at step ', 130000, ': ', 0.32548552763089539)\n",
      "('Average loss at step ', 132000, ': ', 0.3301403536917642)\n",
      "('Average loss at step ', 134000, ': ', 0.31358818348962814)\n",
      "('Average loss at step ', 136000, ': ', 0.32148196951206776)\n",
      "('Average loss at step ', 138000, ': ', 0.30854659063555301)\n",
      "('Average loss at step ', 140000, ': ', 0.30975131610315293)\n",
      "('Average loss at step ', 142000, ': ', 0.3072485857931897)\n",
      "('Average loss at step ', 144000, ': ', 0.298602583585307)\n",
      "('Average loss at step ', 146000, ': ', 0.30100096452143044)\n",
      "('Average loss at step ', 148000, ': ', 0.30451289604883641)\n",
      "('Average loss at step ', 150000, ': ', 0.29785723248217255)\n",
      "('Average loss at step ', 152000, ': ', 0.29442845340725032)\n",
      "('Average loss at step ', 154000, ': ', 0.29254684588406232)\n",
      "('Average loss at step ', 156000, ': ', 0.29339462592452764)\n",
      "('Average loss at step ', 158000, ': ', 0.28427345554344358)\n",
      "('Average loss at step ', 160000, ': ', 0.29411817412916569)\n",
      "('Average loss at step ', 162000, ': ', 0.29477473651524633)\n",
      "('Average loss at step ', 164000, ': ', 0.27838873800169678)\n",
      "('Average loss at step ', 166000, ': ', 0.27763236971944572)\n",
      "('Average loss at step ', 168000, ': ', 0.28172385250404475)\n",
      "('Average loss at step ', 170000, ': ', 0.28286433975072578)\n",
      "('Average loss at step ', 172000, ': ', 0.27546086294669658)\n",
      "('Average loss at step ', 174000, ': ', 0.26997775817662478)\n",
      "('Average loss at step ', 176000, ': ', 0.2664994080821052)\n",
      "('Average loss at step ', 178000, ': ', 0.26347368171904234)\n",
      "('Average loss at step ', 180000, ': ', 0.26815244559338314)\n",
      "('Average loss at step ', 182000, ': ', 0.25718620680784804)\n",
      "('Average loss at step ', 184000, ': ', 0.25931428518751637)\n",
      "('Average loss at step ', 186000, ': ', 0.26294431087281556)\n",
      "('Average loss at step ', 188000, ': ', 0.25964219667389987)\n",
      "('Average loss at step ', 190000, ': ', 0.2581819269331172)\n",
      "('Average loss at step ', 192000, ': ', 0.25727523412462322)\n",
      "('Average loss at step ', 194000, ': ', 0.26254104273673146)\n",
      "('Average loss at step ', 196000, ': ', 0.2474923811177723)\n",
      "('Average loss at step ', 198000, ': ', 0.25615576713252813)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 200000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_batch = 32\n",
    "data_size = vocabulary_size\n",
    "training_size = vocabulary_size - vocabulary_size//10\n",
    "testing_size = data_size - training_size\n",
    "INPUT_FEATURES = 131\n",
    "# start_timestamp\tduration\tstart_geohash\tend_geohash\tstart_timestamp_obj\tstart_of_day\tweekday\thour\tminute\n",
    "\"\"\" Generate the input feature dataset \"\"\"\n",
    "def get_batch(batch_size, df):\n",
    "    global batch_index\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        index = (batch_index + i) % training_size\n",
    "        batch[i, 0] = df.iloc[index][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[index][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[index][\"minute\"]\n",
    "        start_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[index][\"start_geohash\"]]]\n",
    "        end_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[index][\"end_geohash\"]]]\n",
    "        embedding_start_index = 3\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + embedding_start_index] = start_geohash_embedding[j]\n",
    "        end_geohash_emebedding_start = embedding_start_index + embedding_size\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + end_geohash_emebedding_start] = start_geohash_embedding[j]\n",
    "        labels[i, 0] = df.iloc[index][\"duration\"]\n",
    "    batch_index += batch_size % training_size\n",
    "    return batch, labels\n",
    "\n",
    "def get_test_batch(batch_size, df):\n",
    "    indices = []\n",
    "    for i in range(batch_size):\n",
    "        indices.append(random.randrange(0, len(df)))\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "    for i, position in enumerate(indices):\n",
    "        batch[i, 0] = df.iloc[position][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[position][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[position][\"minute\"]\n",
    "        start_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[position][\"start_geohash\"]]]\n",
    "        end_geohash_embedding = final_embeddings[reverse_dictionary[df.iloc[position][\"end_geohash\"]]]\n",
    "        embedding_start_index = 3\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + embedding_start_index] = start_geohash_embedding[j]\n",
    "        end_geohash_emebedding_start = embedding_start_index + embedding_size\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + end_geohash_emebedding_start] = start_geohash_embedding[j]\n",
    "        labels[i, 0] = df.iloc[position][\"duration\"]\n",
    "    return batch, labels\n",
    "\n",
    "def get_full_test_data(limit=None):\n",
    "    path = \"test.csv\"\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    if limit:\n",
    "        df = df[:limit]\n",
    "    df = transform_dataset(df)\n",
    "    batch_size = len(df)\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        batch[i, 0] = df.iloc[i][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[i][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[i][\"minute\"]\n",
    "        start_geohash_index = reverse_dictionary.get(df.iloc[i][\"start_geohash\"], random.randrange(0, 1000))\n",
    "        end_geohash_index = reverse_dictionary.get(df.iloc[i][\"end_geohash\"], random.randrange(0, 1000))\n",
    "        start_geohash_embedding = final_embeddings[start_geohash_index]\n",
    "        end_geohash_embedding = final_embeddings[end_geohash_index]\n",
    "        embedding_start_index = 3\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + embedding_start_index] = start_geohash_embedding[j]\n",
    "        end_geohash_emebedding_start = embedding_start_index + embedding_size\n",
    "        for j in range(embedding_size):\n",
    "            batch[i, j + end_geohash_emebedding_start] = start_geohash_embedding[j]\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_placeholders():\n",
    "    with tf.name_scope('input_layer'):\n",
    "        features = tf.placeholder(tf.float32, shape=[None, INPUT_FEATURES])\n",
    "        train_labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    return features, train_labels\n",
    "def inference(features, hidden1_units, hidden2_units):\n",
    "    \"\"\" Building the tensorflow model \"\"\"\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_FEATURES, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_FEATURES))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features, weights) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, 1],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([1]),\n",
    "                             name='biases')\n",
    "        output_linear = tf.matmul(hidden2, weights) + biases\n",
    "    return output_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_loss(output_linear, labels, n_samples):\n",
    "    \"\"\"Calculates the loss from the output_linear and the labels.\n",
    "    Args:\n",
    "      logits: output_linear tensor, float - [batch_size, 1].\n",
    "      labels: Labels tensor, int32 - [batch_size].\n",
    "    Returns:\n",
    "      loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    labels = tf.to_float(labels)\n",
    "#     cost = tf.reduce_mean(tf.square(output_linear - labels))\n",
    "    cost = tf.reduce_sum(tf.pow(output_linear-labels, 2))/(2*n_samples)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def training(loss, starter_learning_rate, decay_steps):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "    Args:\n",
    "      loss: Loss tensor, from loss().\n",
    "      learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               decay_steps, 0.96, staircase=True)   \n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_evaluation(output_linear, labels):\n",
    "\n",
    "    diff = tf.subtract(output_linear, labels)\n",
    "    return tf.reduce_mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_test_data = batch_data\n",
    "# full_test_data = get_full_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: avg loss = 0.01 (0.011 sec)\n",
      "Mean difference in testing data : -736.751\n",
      "Step 3000: avg loss = 37.45 (0.004 sec)\n",
      "Step 6000: avg loss = 40.51 (0.004 sec)\n",
      "Step 9000: avg loss = 25.49 (0.004 sec)\n",
      "Step 12000: avg loss = 25.69 (0.004 sec)\n",
      "Step 15000: avg loss = 35.50 (0.004 sec)\n",
      "Step 18000: avg loss = 29.58 (0.004 sec)\n",
      "Step 21000: avg loss = 23.86 (0.004 sec)\n",
      "Step 24000: avg loss = 26.72 (0.004 sec)\n",
      "Step 27000: avg loss = 38.24 (0.004 sec)\n",
      "Step 30000: avg loss = 23.09 (0.004 sec)\n",
      "Step 33000: avg loss = 25.29 (0.004 sec)\n",
      "Step 36000: avg loss = 35.00 (0.004 sec)\n",
      "Step 39000: avg loss = 28.68 (0.004 sec)\n",
      "Step 42000: avg loss = 23.73 (0.004 sec)\n",
      "Step 45000: avg loss = 35.20 (0.004 sec)\n",
      "Step 48000: avg loss = 28.75 (0.004 sec)\n",
      "Step 51000: avg loss = 23.59 (0.004 sec)\n",
      "Step 54000: avg loss = 25.82 (0.004 sec)\n",
      "Step 57000: avg loss = 37.77 (0.004 sec)\n",
      "Step 60000: avg loss = 22.94 (0.004 sec)\n",
      "Step 63000: avg loss = 24.37 (0.004 sec)\n",
      "Step 66000: avg loss = 34.07 (0.004 sec)\n",
      "Step 69000: avg loss = 28.26 (0.004 sec)\n",
      "Step 72000: avg loss = 23.04 (0.004 sec)\n",
      "Step 75000: avg loss = 25.43 (0.004 sec)\n",
      "Step 78000: avg loss = 37.04 (0.004 sec)\n",
      "Step 81000: avg loss = 22.36 (0.004 sec)\n",
      "Step 84000: avg loss = 24.06 (0.004 sec)\n",
      "Step 87000: avg loss = 34.08 (0.004 sec)\n",
      "Step 90000: avg loss = 27.11 (0.005 sec)\n",
      "Step 93000: avg loss = 22.69 (0.004 sec)\n",
      "Step 96000: avg loss = 34.11 (0.004 sec)\n",
      "Step 99000: avg loss = 27.63 (0.004 sec)\n",
      "Step 102000: avg loss = 22.64 (0.004 sec)\n",
      "Step 105000: avg loss = 24.66 (0.004 sec)\n",
      "Step 108000: avg loss = 36.81 (0.004 sec)\n",
      "Step 111000: avg loss = 22.04 (0.004 sec)\n",
      "Step 114000: avg loss = 23.79 (0.004 sec)\n",
      "Step 117000: avg loss = 32.96 (0.004 sec)\n",
      "Step 120000: avg loss = 27.56 (0.004 sec)\n",
      "Step 123000: avg loss = 22.42 (0.004 sec)\n",
      "Step 126000: avg loss = 24.51 (0.004 sec)\n",
      "Step 129000: avg loss = 36.50 (0.004 sec)\n",
      "Step 132000: avg loss = 22.11 (0.004 sec)\n",
      "Step 135000: avg loss = 23.17 (0.004 sec)\n",
      "Step 138000: avg loss = 33.62 (0.004 sec)\n",
      "Step 141000: avg loss = 26.16 (0.004 sec)\n",
      "Step 144000: avg loss = 22.23 (0.004 sec)\n",
      "Step 147000: avg loss = 33.39 (0.004 sec)\n"
     ]
    }
   ],
   "source": [
    "# data_size = vocabulary_size\n",
    "# training_size = vocabulary_size - vocabulary_size//10\n",
    "# testing_size = data_size - training_size\n",
    "final_results = None\n",
    "batch_index = 0\n",
    "num_steps = 150000\n",
    "input_batch = 4\n",
    "starter_learning_rate = 0.001\n",
    "decay_steps = 10000\n",
    "def start_training(num_steps):\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     tf.global_variables_initializer().run()\n",
    "    with tf.Graph().as_default():\n",
    "        features, labels = input_placeholders()\n",
    "        linear_inference = inference(features, 128, 64)\n",
    "        loss = get_loss(linear_inference, labels, training_size)\n",
    "        train_op = training(loss, starter_learning_rate, decay_steps)\n",
    "        evaluation = get_evaluation(linear_inference, labels)\n",
    "        summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/rides/\", sess.graph)\n",
    "        sess.run(init)\n",
    "        training_data = df[:training_size]\n",
    "        test_data = df[testing_size + 1:]\n",
    "        avg_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            start_time = time.time()\n",
    "            batch_data, label_data = get_batch(input_batch, training_data)\n",
    "            feed_dict = {features:batch_data, labels:label_data}\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                               feed_dict=feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            avg_loss += loss_value\n",
    "\n",
    "          # Write the summaries and print an overview fairly often.\n",
    "            if step % 3000 == 0:\n",
    "                # Print status to stdout.\n",
    "                print('Step %d: avg loss = %.2f (%.3f sec)' % (step, avg_loss/2000, duration))\n",
    "                avg_loss = 0\n",
    "                # Update the events file.\n",
    "#                 summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "#                 summary_writer.add_summary(summary_str, step)\n",
    "#                 summary_writer.flush()\n",
    "            if step % (num_steps - 1) == 0:\n",
    "                test_batch_data, test_label_data = get_test_batch(20000, training_data)\n",
    "                feed_dict = {features:test_batch_data, labels:test_label_data}\n",
    "                results = sess.run(evaluation, feed_dict=feed_dict)\n",
    "                print 'Mean difference in testing data : %s' % results\n",
    "#         batch_data = get_full_test_data()\n",
    "        feed_dict = {features:full_test_data}\n",
    "        global final_results\n",
    "        final_results = sess.run(linear_inference, feed_dict=feed_dict)\n",
    "start_training(num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 250.06097412],\n",
       "       [ 340.99884033],\n",
       "       [ 337.78717041],\n",
       "       [ 196.7807312 ],\n",
       "       [ 639.41094971],\n",
       "       [ 827.86187744],\n",
       "       [ 868.50714111],\n",
       "       [ 796.97949219],\n",
       "       [ 741.89941406],\n",
       "       [ 802.07250977]], dtype=float32)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[:1]\n",
    "final_output = pd.DataFrame({'duration':final_results[:,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_output[:1]\n",
    "final_output.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/mchirico/bcc376fb336b73f24b29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def assert_geohashes_in_set(df, reverse_dictionary):\n",
    "    for index, row in df.iterrows():\n",
    "        missing = set()\n",
    "        if not reverse_dictionary.get(row[\"start_geohash\"]):\n",
    "            missing.add(row[\"start_geohash\"])\n",
    "        if not reverse_dictionary.get(row[\"end_geohash\"]):\n",
    "            missing.add(row[\"end_geohash\"])\n",
    "        if len(missing) > 0:\n",
    "            raise Exception(\"end_geohash is not in global dataset: %s\"% len(missing))\n",
    "\n",
    "def transform_input():\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    df['start_geohash'] = df.apply(geohash_encode_start, axis=1)\n",
    "    print 'end geohash'\n",
    "    df['end_geohash'] = df.apply(geohash_encode_end, axis=1)    \n",
    "#     df[\"start_timestamp_obj\"] = df.apply(get_local_time, axis=1)\n",
    "#     df.to_csv(\"test_mod.csv\")\n",
    "#     df[\"start_of_day\"] = df.apply(get_start_of_the_day, axis=1)\n",
    "#     df[\"weekday\"] = df.apply(get_weekday, axis=1)\n",
    "#     df[\"hour\"] = df.apply(get_hour, axis=1)\n",
    "#     df[\"minute\"] = df.apply(get_min, axis=1)\n",
    "    return df\n",
    "\n",
    "def handle_test_data():\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    df = transform_input(df)\n",
    "    assert_geohashes_in_set(df, reverse_dictionary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "end_geohash is not in global dataset: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-ed8a8f1e3703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dftest = transform_input()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# assert_geohashes_in_set(dftest, reverse_dictionary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0massert_geohashes_in_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-261-4adbaa7bf1e5>\u001b[0m in \u001b[0;36massert_geohashes_in_set\u001b[0;34m(df, reverse_dictionary)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_geohash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"end_geohash is not in global dataset: %s\"\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: end_geohash is not in global dataset: 1"
     ]
    }
   ],
   "source": [
    "# dftest = transform_input()\n",
    "# assert_geohashes_in_set(dftest, reverse_dictionary)\n",
    "assert_geohashes_in_set(dftest, reverse_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
