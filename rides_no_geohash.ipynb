{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import geohash\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # references\n",
    "# # https://github.com/nlintz/TensorFlow-Tutorials\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py\n",
    "# # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py    \n",
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/estimators/abalone.py\n",
    "# https://github.com/lyft/challenge/blob/master/data_scientist/travel_time/description/travel_time.pdf\n",
    "# linear regression https://aqibsaeed.github.io/2016-07-07-TensorflowLR/\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/2_BasicModels/linear_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# raw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n",
    "#         'last_name': ['Miller', 'Jacobson', \".\", 'Milner', 'Cooze'],\n",
    "#         'age': [42, 52, 36, 24, 73],\n",
    "#         'preTestScore': [4, 24, 31, \".\", \".\"],\n",
    "#         'postTestScore': [\"25,000\", \"94,000\", 57, 62, 70]}\n",
    "# df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore'])\n",
    "# df\n",
    "inputdf = pd.read_csv(\"train.csv\")\n",
    "df = inputdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_training_data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-73.993111</td>\n",
       "      <td>40.724289</td>\n",
       "      <td>-74.000977</td>\n",
       "      <td>40.735222</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-73.971924</td>\n",
       "      <td>40.762749</td>\n",
       "      <td>-73.965698</td>\n",
       "      <td>40.771427</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-73.953247</td>\n",
       "      <td>40.765816</td>\n",
       "      <td>-73.952843</td>\n",
       "      <td>40.772453</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-73.986618</td>\n",
       "      <td>40.739353</td>\n",
       "      <td>-73.949158</td>\n",
       "      <td>40.805161</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-73.968864</td>\n",
       "      <td>40.757317</td>\n",
       "      <td>-73.982521</td>\n",
       "      <td>40.771305</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>-73.986465</td>\n",
       "      <td>40.739922</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.721848</td>\n",
       "      <td>20</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>-73.963791</td>\n",
       "      <td>40.763790</td>\n",
       "      <td>-73.995850</td>\n",
       "      <td>40.741783</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>-73.999825</td>\n",
       "      <td>40.743290</td>\n",
       "      <td>-74.001366</td>\n",
       "      <td>40.756424</td>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>-73.955147</td>\n",
       "      <td>40.765026</td>\n",
       "      <td>-73.990143</td>\n",
       "      <td>40.751049</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>-73.980156</td>\n",
       "      <td>40.760777</td>\n",
       "      <td>-73.970421</td>\n",
       "      <td>40.764828</td>\n",
       "      <td>13</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  start_lng  start_lat    end_lng    end_lat  hour  minute  weekday\n",
       "0       0 -73.993111  40.724289 -74.000977  40.735222     3      13        6\n",
       "1       1 -73.971924  40.762749 -73.965698  40.771427    13       2        1\n",
       "2       2 -73.953247  40.765816 -73.952843  40.772453    13       2        1\n",
       "3       3 -73.986618  40.739353 -73.949158  40.805161     4       8        3\n",
       "4       4 -73.968864  40.757317 -73.982521  40.771305    23       9        2\n",
       "5       5 -73.986465  40.739922 -73.979027  40.721848    20      41        3\n",
       "6       6 -73.963791  40.763790 -73.995850  40.741783    22      37        5\n",
       "7       7 -73.999825  40.743290 -74.001366  40.756424    11      58        5\n",
       "8       8 -73.955147  40.765026 -73.990143  40.751049    23      22        2\n",
       "9       9 -73.980156  40.760777 -73.970421  40.764828    13      56        3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "### CONSTRUCT FINAL DATAFRAME FROM INPUT\n",
    "\n",
    "def transform_dataset(df):\n",
    "    tz = pytz.timezone('America/New_York')\n",
    "    def get_local_time(timestamp):\n",
    "        d = datetime.utcfromtimestamp(timestamp).replace(tzinfo=pytz.utc)\n",
    "        dt = d.astimezone(tz)\n",
    "        return dt\n",
    "    \n",
    "#     d[\"geo\"] = d[\"start_geohash\"] + \":\" + d[\"end_geohash\"]\n",
    "    start_time = df[\"start_timestamp\"].apply(get_local_time)\n",
    "    start_time.name = 'start_timestamp_obj'\n",
    "    df = pd.concat([df, start_time], axis = 1)    \n",
    "    # start_time[:10]\n",
    "\n",
    "    hour = df[\"start_timestamp_obj\"].dt.hour\n",
    "    hour.name = 'hour'\n",
    "    df = pd.concat([df, hour], axis = 1)\n",
    "\n",
    "    minute = df[\"start_timestamp_obj\"].dt.minute\n",
    "    minute.name = 'minute'\n",
    "    df = pd.concat([df, minute], axis = 1)\n",
    "\n",
    "    weekday = df[\"start_timestamp_obj\"].dt.weekday\n",
    "    weekday.name = 'weekday'\n",
    "    df = pd.concat([df, weekday], axis = 1)\n",
    "\n",
    "    del start_time\n",
    "    del hour\n",
    "    del minute\n",
    "    del weekday\n",
    "    outdf = df.copy()\n",
    "    outdf.drop('start_timestamp', axis=1, inplace=True)\n",
    "    outdf.drop('start_timestamp_obj', axis=1, inplace=True)\n",
    "    return outdf\n",
    "#     df = pd.concat([df, pd.DataFrame(start_time, index=df.index)], axis = 1)\n",
    "dfff = pd.read_csv(\"test.csv\")\n",
    "d = dfff[:100].copy()\n",
    "d = transform_dataset(d)\n",
    "d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = transform_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>duration</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>112</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  start_lng  start_lat    end_lng    end_lat  duration  hour  minute  \\\n",
       "0       0 -74.009087  40.713818 -74.004326  40.719986       112    23      33   \n",
       "\n",
       "   weekday  \n",
       "0        5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.to_csv(\"train_mod.csv\", index=False)\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# g = set()\n",
    "# dictionary = {}\n",
    "# reverse_dictionary = {}\n",
    "# for index, row in df.iterrows():\n",
    "#     g.add(row['start_geohash'])\n",
    "#     g.add(row['end_geohash'])\n",
    "# for i, item in enumerate(g):\n",
    "#     dictionary[i] = item\n",
    "#     reverse_dictionary[item] = i\n",
    "\n",
    "# tup = defaultdict(list)\n",
    "# for index, row in df.iterrows():\n",
    "#     tup[row['start_geohash']].append((row['duration'], row['end_geohash']))\n",
    "# glist = list(g)\n",
    "    \n",
    "# #     gmap[row['start_geohash']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# dictionary = {}\n",
    "# reverse_dictionary = {}\n",
    "# start_list = list(df['start_geohash'])\n",
    "# end_list = list(df['end_geohash'])\n",
    "# geoset = set(start_list + end_list)\n",
    "\n",
    "# for i, item in enumerate(geoset):\n",
    "#     dictionary[i] = item\n",
    "#     reverse_dictionary[item] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tup = defaultdict(list)\n",
    "# def geo_mapping(combo):\n",
    "#     start, end, duration = combo.split(\":\")\n",
    "#     tup[start].append((int(duration), end))\n",
    "    \n",
    "# # df[\"geo\"] = df[\"start_geohash\"] + \":\" + df[\"end_geohash\"] + \":\" + df[\"duration\"].astype(str)\n",
    "# _ = df[\"geo\"].apply(geo_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sorted_tup = {}\n",
    "# for k, v in tup.iteritems():\n",
    "#     sorted_tup[k] = sorted(tup[k])[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dict_head(d, num=10):\n",
    "    i = 0\n",
    "    for k, v in d.iteritems():\n",
    "        print k, v\n",
    "        i += 1\n",
    "        if i > num:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch [ 0  2  4  5  6  9 15 21 23 34]\n",
      "labels [[    0]\n",
      " [ 7645]\n",
      " [    4]\n",
      " [15182]\n",
      " [20409]\n",
      " [39681]\n",
      " [   15]\n",
      " [ 6454]\n",
      " [14170]\n",
      " [ 8020]]\n"
     ]
    }
   ],
   "source": [
    "# ### BEGINNING OF THE EMBEDDINGS MODEL\n",
    "\n",
    "# def generate_batch(batch_size):\n",
    "#     global data_index\n",
    "#     batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "#     labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "#     dictionary[data_index]\n",
    "#     i = 0\n",
    "#     while i < batch_size:\n",
    "#         item = data_index\n",
    "#         element = sorted_tup.get(dictionary[data_index])\n",
    "#         if not element:\n",
    "#             data_index = (data_index + 1) % len(sorted_tup)\n",
    "#             continue\n",
    "#         options = sorted_tup[dictionary[data_index]]\n",
    "#         choice = random.choice(options[:300])\n",
    "#         batch[i] = item\n",
    "#         labels[i, 0] = reverse_dictionary[choice[1]]\n",
    "#         data_index = (data_index + 1) % len(sorted_tup)\n",
    "#         i += 1\n",
    "#     return batch, labels\n",
    "# batch, labels = generate_batch(10)\n",
    "# print 'batch', batch\n",
    "# print 'labels', labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# batch_size = 512\n",
    "# embedding_size = 8  # Dimension of the embedding vector.\n",
    "# num_sampled = 256\n",
    "# vocabulary_size = len(geoset)\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# with graph.as_default():\n",
    "\n",
    "#   # Input data.\n",
    "#     train_inputs = tf.placeholder(tf.int64, shape=[batch_size])\n",
    "#     train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])\n",
    "\n",
    "#     # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "#     with tf.device('/cpu:0'):\n",
    "#     # Look up embeddings for inputs.\n",
    "#         embeddings = tf.Variable(\n",
    "#             tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "#         embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "#         # Construct the variables for the NCE loss\n",
    "#         nce_weights = tf.Variable(\n",
    "#             tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "#                                 stddev=1.0 / math.sqrt(embedding_size)))\n",
    "#         nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "#   # Compute the average NCE loss for the batch.\n",
    "#   # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "#   # time we evaluate the loss.\n",
    "#     loss = tf.reduce_mean(\n",
    "#       tf.nn.nce_loss(weights=nce_weights,\n",
    "#                      biases=nce_biases,\n",
    "#                      labels=train_labels,\n",
    "#                      inputs=embed,\n",
    "#                      num_sampled=num_sampled,\n",
    "#                      num_classes=vocabulary_size))\n",
    "\n",
    "#     # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "#     # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "#     norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "#     normalized_embeddings = embeddings / norm\n",
    "# #     valid_embeddings = tf.nn.embedding_lookup(\n",
    "# #       normalized_embeddings, valid_dataset)\n",
    "# #     similarity = tf.matmul(\n",
    "# #       valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "#     init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 927.4444580078125)\n",
      "('Average loss at step ', 2000, ': ', 244.21554810619355)\n",
      "('Average loss at step ', 4000, ': ', 21.559425556182862)\n",
      "('Average loss at step ', 6000, ': ', 6.8963463888168333)\n",
      "('Average loss at step ', 8000, ': ', 4.5532505497932432)\n",
      "('Average loss at step ', 10000, ': ', 3.9644158512353895)\n",
      "('Average loss at step ', 12000, ': ', 3.6713721165657045)\n",
      "('Average loss at step ', 14000, ': ', 3.4193316249847414)\n",
      "('Average loss at step ', 16000, ': ', 3.210755734682083)\n",
      "('Average loss at step ', 18000, ': ', 3.0320033909082413)\n",
      "('Average loss at step ', 20000, ': ', 2.8759165284633634)\n",
      "('Average loss at step ', 22000, ': ', 2.7272590008974076)\n",
      "('Average loss at step ', 24000, ': ', 2.6201186646223067)\n",
      "('Average loss at step ', 26000, ': ', 2.5232962066531179)\n",
      "('Average loss at step ', 28000, ': ', 2.4241663871407511)\n",
      "('Average loss at step ', 30000, ': ', 2.3475983265042304)\n",
      "('Average loss at step ', 32000, ': ', 2.286267070353031)\n",
      "('Average loss at step ', 34000, ': ', 2.2265191638469695)\n",
      "('Average loss at step ', 36000, ': ', 2.1669505226612089)\n",
      "('Average loss at step ', 38000, ': ', 2.1163795489072799)\n",
      "('Average loss at step ', 40000, ': ', 2.0681821384429933)\n",
      "('Average loss at step ', 42000, ': ', 2.0164086657166482)\n",
      "('Average loss at step ', 44000, ': ', 1.9737530205249787)\n",
      "('Average loss at step ', 46000, ': ', 1.9398199190497398)\n",
      "('Average loss at step ', 48000, ': ', 1.9027875819206237)\n",
      "('Average loss at step ', 50000, ': ', 1.8642524707913399)\n",
      "('Average loss at step ', 52000, ': ', 1.8319674357175828)\n",
      "('Average loss at step ', 54000, ': ', 1.8167254586815833)\n",
      "('Average loss at step ', 56000, ': ', 1.7770193762779236)\n",
      "('Average loss at step ', 58000, ': ', 1.7447241187691689)\n",
      "('Average loss at step ', 60000, ': ', 1.7271413068771362)\n",
      "('Average loss at step ', 62000, ': ', 1.6965730926394462)\n",
      "('Average loss at step ', 64000, ': ', 1.6734828062057494)\n",
      "('Average loss at step ', 66000, ': ', 1.6432909252047538)\n",
      "('Average loss at step ', 68000, ': ', 1.634077120602131)\n",
      "('Average loss at step ', 70000, ': ', 1.6114283032119274)\n",
      "('Average loss at step ', 72000, ': ', 1.5909714853167534)\n",
      "('Average loss at step ', 74000, ': ', 1.5658653959929942)\n",
      "('Average loss at step ', 76000, ': ', 1.5475259523093701)\n",
      "('Average loss at step ', 78000, ': ', 1.5360726554393769)\n",
      "('Average loss at step ', 80000, ': ', 1.5225373172163963)\n",
      "('Average loss at step ', 82000, ': ', 1.4989326479136944)\n",
      "('Average loss at step ', 84000, ': ', 1.484089328199625)\n",
      "('Average loss at step ', 86000, ': ', 1.4746610824465751)\n",
      "('Average loss at step ', 88000, ': ', 1.458870416343212)\n",
      "('Average loss at step ', 90000, ': ', 1.4336110525727273)\n",
      "('Average loss at step ', 92000, ': ', 1.4307674587070942)\n",
      "('Average loss at step ', 94000, ': ', 1.418707862764597)\n",
      "('Average loss at step ', 96000, ': ', 1.3963092332482339)\n",
      "('Average loss at step ', 98000, ': ', 1.3886489651203155)\n"
     ]
    }
   ],
   "source": [
    "# # Step 5: Begin training.\n",
    "# num_steps = 200000\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#     # We must initialize all variables before we use them.\n",
    "#     init.run()\n",
    "#     print('Initialized')\n",
    "\n",
    "#     average_loss = 0\n",
    "#     for step in xrange(num_steps):\n",
    "#         batch_inputs, batch_labels = generate_batch(\n",
    "#             batch_size)\n",
    "#         feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "#         # We perform one update step by evaluating the optimizer op (including it\n",
    "#         # in the list of returned values for session.run()\n",
    "#         _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "#         average_loss += loss_val\n",
    "\n",
    "#         if step % 2000 == 0:\n",
    "#             if step > 0:\n",
    "#                 average_loss /= 2000\n",
    "#             # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "#             print('Average loss at step ', step, ': ', average_loss)\n",
    "#             average_loss = 0\n",
    "\n",
    "#     final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_batch = 32\n",
    "data_size = len(df)\n",
    "training_size = data_size - data_size//10\n",
    "testing_size = data_size - training_size\n",
    "INPUT_FEATURES = 7\n",
    "# start_timestamp\tduration\tstart_geohash\tend_geohash\tstart_timestamp_obj\tstart_of_day\tweekday\thour\tminute\n",
    "\"\"\" Generate the input feature dataset \"\"\"\n",
    "def get_batch(batch_size, df):\n",
    "    global batch_index\n",
    "    skip = random.randrange(0, batch_size)\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "    forward_increment = 0\n",
    "    for i in range(batch_size):\n",
    "        increase_forward = random.randint(0, 3)\n",
    "        forward_increment += increase_forward\n",
    "        index = (batch_index + i + skip + forward_increment) % training_size\n",
    "        batch[i, 0] = df.iloc[index][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[index][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[index][\"minute\"]\n",
    "        batch[i, 3] = df.iloc[index][\"start_lat\"]\n",
    "        batch[i, 4] = df.iloc[index][\"start_lng\"]\n",
    "        batch[i, 5] = df.iloc[index][\"end_lat\"]\n",
    "        batch[i, 6] = df.iloc[index][\"end_lng\"]\n",
    "        labels[i, 0] = df.iloc[index][\"duration\"]\n",
    "    batch_index += batch_size % training_size\n",
    "    return batch, labels\n",
    "\n",
    "def get_all_batches(batch_size, input_df):\n",
    "    num_batches = len(input_df) // batch_size\n",
    "    batches = []\n",
    "    index = 0\n",
    "    for i in range(num_batches):\n",
    "        if index + batch_size >= len(input_df):\n",
    "            break\n",
    "        df = input_df[index: index + batch_size]\n",
    "        batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "        batch[:, 0] = df[\"weekday\"]\n",
    "        batch[:, 1] = df[\"hour\"]\n",
    "        batch[:, 2] = df[\"minute\"]\n",
    "        batch[:, 3] = df[\"start_lat\"]\n",
    "        batch[:, 4] = df[\"start_lng\"]\n",
    "        batch[:, 5] = df[\"end_lat\"]\n",
    "        batch[:, 6] = df[\"end_lng\"]   \n",
    "        labels[:, 0] = df[\"duration\"]\n",
    "        batches.append((batch, labels))\n",
    "        index += batch_size\n",
    "    return batches\n",
    "\n",
    "\n",
    "def get_test_batch(batch_size, df):\n",
    "    indices = []\n",
    "    for i in range(batch_size):\n",
    "        indices.append(random.randrange(0, len(df)))\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n",
    "    for i, position in enumerate(indices):\n",
    "        batch[i, 0] = df.iloc[position][\"weekday\"]\n",
    "        batch[i, 1] = df.iloc[position][\"hour\"]\n",
    "        batch[i, 2] = df.iloc[position][\"minute\"]\n",
    "        batch[i, 3] = df.iloc[position][\"start_lat\"]\n",
    "        batch[i, 4] = df.iloc[position][\"start_lng\"]\n",
    "        batch[i, 5] = df.iloc[position][\"end_lat\"]\n",
    "        batch[i, 6] = df.iloc[position][\"end_lng\"]\n",
    "        labels[i, 0] = df.iloc[position][\"duration\"]\n",
    "    return batch, labels\n",
    "\n",
    "def get_full_test_data(limit=None):\n",
    "    path = \"test.csv\"\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    if limit:\n",
    "        df = df[:limit]\n",
    "    df = transform_dataset(df)\n",
    "    batch_size = len(df)\n",
    "    batch = np.ndarray(shape=(batch_size, INPUT_FEATURES), dtype=np.float32)\n",
    "    batch[:, 0] = df[\"weekday\"]\n",
    "    batch[:, 1] = df[\"hour\"]\n",
    "    batch[:, 2] = df[\"minute\"]\n",
    "    batch[:, 3] = df[\"start_lat\"]\n",
    "    batch[:, 4] = df[\"start_lng\"]\n",
    "    batch[:, 5] = df[\"end_lat\"]\n",
    "    batch[:, 6] = df[\"end_lng\"]             \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_placeholders():\n",
    "    with tf.name_scope('input_layer'):\n",
    "        features = tf.placeholder(tf.float32, shape=[None, INPUT_FEATURES])\n",
    "        train_labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    return features, train_labels\n",
    "def inference(features, hidden1_units, hidden2_units):\n",
    "    \"\"\" Building the tensorflow model \"\"\"\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights_1 = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_FEATURES, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_FEATURES))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features, weights_1) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights_2 = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights_2) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, 1],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([1]),\n",
    "                             name='biases')\n",
    "        output_linear = tf.matmul(hidden2, weights) + biases\n",
    "    return output_linear, (weights, weights_1, weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-cb188f605727>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-cb188f605727>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    return cost\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def get_loss(output_linear, labels, n_samples):\n",
    "#     \"\"\"Calculates the loss from the output_linear and the labels.\n",
    "#     Args:\n",
    "#       logits: output_linear tensor, float - [batch_size, 1].\n",
    "#       labels: Labels tensor, int32 - [batch_size].\n",
    "#     Returns:\n",
    "#       loss: Loss tensor of type float.\n",
    "#     \"\"\"\n",
    "#     labels = tf.to_float(labels)\n",
    "# #     cost = tf.reduce_mean(tf.square((output_linear - labels)/labels))\n",
    "#     cost = tf.reduce_sum(tf.pow(output_linear-labels, 2))/(2*n_samples)\n",
    "#     return cost\n",
    "\n",
    "def get_loss(output_linear, labels, n_samples):\n",
    "    \"\"\"Calculates the loss from the output_linear and the labels.\n",
    "    Args:\n",
    "      logits: output_linear tensor, float - [batch_size, 1].\n",
    "      labels: Labels tensor, int32 - [batch_size].\n",
    "    Returns:\n",
    "      loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "#     cost = tf.reduce_mean(tf.pow(output_linear-labels, 2))/(2*n_samples)\n",
    "    cost = tf.reduce_sum(tf.abs((output_linear-labels)/labels)/(2)\n",
    "#     reduction = tf.square(tf.subtract(output_linear, labels))\n",
    "#     red = tf.reduce_mean(reduction)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def training(loss, starter_learning_rate, decay_steps):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "    Args:\n",
    "      loss: Loss tensor, from loss().\n",
    "      learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "      train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               decay_steps, 0.96, staircase=True)   \n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_evaluation(output_linear, labels):\n",
    "\n",
    "    diff = tf.abs(tf.subtract(output_linear, labels))\n",
    "    return tf.reduce_mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full_test_data = batch_data\n",
    "full_test_data = get_full_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(full_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: avg loss = 0.00004 (0.017 sec)\n",
      "Mean difference in testing data : 846.172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-35e943ef5029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mfinal_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-163-35e943ef5029>\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(num_steps)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             _, loss_value = sess.run([train_op, reg_loss],\n",
      "\u001b[0;32m<ipython-input-125-62cc3c734593>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(batch_size, df)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_lat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_lng\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"duration\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbatch_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtraining_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1628\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_get_loc\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   1984\u001b[0m                                                       \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m                                                       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m                                                       dtype=new_values.dtype)\n\u001b[0m\u001b[1;32m   1987\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_validate_dtype\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_coerce_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# a compound dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/types/common.pyc\u001b[0m in \u001b[0;36m_coerce_to_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_coerce_to_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;34m\"\"\" coerce a string / np.dtype to a dtype \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/types/common.pyc\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/rides/venv/local/lib/python2.7/site-packages/pandas/types/dtypes.pyc\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data_size = vocabulary_size\n",
    "# training_size = vocabulary_size - vocabulary_size//10\n",
    "# testing_size = data_size - training_size\n",
    "final_results = None\n",
    "batch_index = 0\n",
    "num_steps = 50000\n",
    "input_batch = 4\n",
    "starter_learning_rate = 0.01\n",
    "decay_steps = 1000\n",
    "beta = 0.001\n",
    "def start_training(num_steps):\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     tf.global_variables_initializer().run()\n",
    "    with tf.Graph().as_default():\n",
    "        features, labels = input_placeholders()\n",
    "        linear_inference, (w, w1, w2) = inference(features, 16, 8)\n",
    "        loss = get_loss(linear_inference, labels, training_size)\n",
    "        reg_loss = loss + beta*tf.nn.l2_loss(w1) + beta*tf.nn.l2_loss(w2)\n",
    "        train_op = training(reg_loss, starter_learning_rate, decay_steps)\n",
    "        evaluation = get_evaluation(linear_inference, labels)\n",
    "        summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/rides/\", sess.graph)\n",
    "        sess.run(init)\n",
    "        training_data = df[:training_size]\n",
    "        test_data = df[testing_size + 1:]\n",
    "        avg_loss = 0\n",
    "        validation_data, validation_labels = get_test_batch(10000, test_data)\n",
    "        for step in xrange(num_steps):\n",
    "            start_time = time.time()\n",
    "            batch_data, label_data = get_batch(input_batch, training_data)\n",
    "            feed_dict = {features:batch_data, labels:label_data}\n",
    "            _, loss_value = sess.run([train_op, reg_loss],\n",
    "                               feed_dict=feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            avg_loss += loss_value\n",
    "\n",
    "          # Write the summaries and print an overview fairly often.\n",
    "            if step % 2000 == 0:\n",
    "                # Print status to stdout.\n",
    "                print('Step %d: avg loss = %.5f (%.3f sec)' % (step, avg_loss/2000, duration))\n",
    "                avg_loss = 0\n",
    "                # Update the events file.\n",
    "#                 summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "#                 summary_writer.add_summary(summary_str, step)\n",
    "#                 summary_writer.flush()\n",
    "            if step % 6000 == 0:\n",
    "#                 test_batch_data, test_label_data = get_test_batch(5000, training_data)\n",
    "                feed_dict = {features:validation_data, labels:validation_labels}\n",
    "                results = sess.run(evaluation, feed_dict=feed_dict)\n",
    "                print 'Mean difference in testing data : %s' % results\n",
    "#         batch_data = get_full_test_data()\n",
    "        feed_dict = {features:full_test_data}\n",
    "        global final_results\n",
    "        final_results = sess.run(linear_inference, feed_dict=feed_dict)\n",
    "start_training(num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Batch normalization reference\n",
    "# http://ruishu.io/2016/12/27/batchnorm/\n",
    "# http://r2rt.com/implementing-batch-normalization-in-tensorflow.html#making-predictions-with-the-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dense(x, size, scope):\n",
    "    return tf.contrib.layers.fully_connected(x, size, \n",
    "                                             activation_fn=None,\n",
    "                                             scope=scope)\n",
    "\n",
    "def dense_batch_relu(input_num, output_num, input_mat, phase, scope):\n",
    "    with tf.name_scope(scope):\n",
    "        weights_1 = tf.Variable(\n",
    "            tf.truncated_normal([input_num, output_num],\n",
    "                                stddev=1.0 / math.sqrt(float(input_num))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([output_num]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(input_mat, weights_1) + biases)\n",
    "        return hidden1\n",
    "        \n",
    "# def dense_batch_relu(x, phase, scope):\n",
    "#     with tf.variable_scope(scope):\n",
    "# #         h1 = tf.contrib.layers.fully_connected(x, 64, \n",
    "# #                                                activation_fn=None,\n",
    "# #                                                scope='dense')\n",
    "#         weights_1 = tf.Variable(\n",
    "#             tf.truncated_normal([x, 32],\n",
    "#                                 stddev=1.0 / math.sqrt(float(x))),\n",
    "#             name='weights')\n",
    "#         biases = tf.Variable(tf.zeros([32]),\n",
    "#                              name='biases')\n",
    "#         hidden1 = tf.nn.relu(tf.matmul(features, weights_1) + biases)        \n",
    "# #         h2 = tf.contrib.layers.batch_norm(h1, \n",
    "# #                                           center=True, scale=True, \n",
    "# #                                           is_training=phase,\n",
    "# #                                           scope='bn')\n",
    "#         return hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_batch = 64\n",
    "tf.reset_default_graph()\n",
    "beta = tf.constant(0.01, dtype='float32')\n",
    "train_batch = tf.placeholder('float32', (None, INPUT_FEATURES), name='train_batch')\n",
    "y = tf.placeholder('float32', (None, 1), name='y')\n",
    "validation_labels = tf.placeholder('float32', (None, 1), name='validation_labels')\n",
    "test_batch = tf.placeholder('float32', (None, 1), name='test_labels')\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "n_samples = tf.constant(64000.0, dtype='float32')\n",
    "h1 = dense_batch_relu(INPUT_FEATURES, 32, train_batch, phase, 'layer1')\n",
    "h2 = dense_batch_relu(32, 32, h1, phase, 'layer2')\n",
    "# h1 = dense_batch_relu(train_batch, phase,'layer1')\n",
    "# h2 = dense_batch_relu(h1, phase, 'layer2')\n",
    "# linear = dense(h2, 1, 'linear')\n",
    "linear = dense_batch_relu(32, 10, h2, phase, 'linear')\n",
    "\n",
    "evaluation = tf.reduce_mean(tf.abs(tf.subtract(linear, validation_labels)))\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.abs(linear-y)) + beta*tf.nn.l2_loss(linear) + beta*tf.nn.l2_loss(h1) + beta*tf.nn.l2_loss(h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data = df[:training_size]\n",
    "test_data = df[testing_size + 1:]\n",
    "batches = get_all_batches(input_batch, training_data)\n",
    "batch_index = 0\n",
    "batch_list_len = len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterep  20000\n",
      "Step 3999: avg loss = 70874.4176\n",
      "Step 7999: avg loss = 743.3454\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'h1' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-8b8e3617ffec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                             'phase:0': 0})\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mfinal_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-200-8b8e3617ffec>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                     \u001b[0;34m'validation_labels:0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                     'phase:0': 0})\n\u001b[0;32m---> 38\u001b[0;31m             h1 = sess.run(h1, \n\u001b[0m\u001b[1;32m     39\u001b[0m                          feed_dict={'train_batch:0': validation_data,\n\u001b[1;32m     40\u001b[0m                                     \u001b[0;34m'validation_labels:0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'h1' referenced before assignment"
     ]
    }
   ],
   "source": [
    "validation_data, validation_labels = get_test_batch(10000, test_data)\n",
    "\n",
    "def train():\n",
    "#     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#     with tf.control_dependencies(update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the train_step\n",
    "#         train_step = tf.train.AdamOptimizer(1.0).minimize(loss)\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)        \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     training_data = df[:training_size]\n",
    "#     batches = get_all_batches(input_batch, training_data)\n",
    "#     batch_index = 0\n",
    "#     batch_list_len = len(batches)\n",
    "#     test_data = df[testing_size + 1:]\n",
    "    avg_loss = 0\n",
    "#     validation_data, validation_labels = get_test_batch(10000, test_data)\n",
    "#     iterep = len(df)//(input_batch*500)\n",
    "    iterep = 20000\n",
    "    print 'iterep ', iterep\n",
    "    for i in range(iterep * 200):\n",
    "#         batch_data, label_data = get_batch(input_batch, training_data)\n",
    "        batch_index = i % batch_list_len\n",
    "        batch_data, label_data = batches[batch_index]\n",
    "        _, loss_value = sess.run([train_step, loss],\n",
    "                 feed_dict={'train_batch:0': batch_data, \n",
    "                            'y:0': label_data, \n",
    "                            'phase:0': 1})\n",
    "        avg_loss += loss_value\n",
    "        if (i + 1) %  4000 == 0:\n",
    "            print('Step %d: avg loss = %.4f' % (i, avg_loss/4000))            \n",
    "            avg_loss = 0\n",
    "        if (i + 1) %  10000 == 0: \n",
    "            t = sess.run(evaluation, \n",
    "                         feed_dict={'train_batch:0': validation_data,\n",
    "                                    'validation_labels:0': validation_labels,\n",
    "                                    'phase:0': 0})\n",
    "            print('Step %d: evaluation on validation set = %0.1f' % (i, t))\n",
    "    final_results = sess.run(linear, \n",
    "                 feed_dict={'train_batch:0': full_test_data,\n",
    "                            'phase:0': 0})\n",
    "    return final_results\n",
    "final_results = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 822.66094971],\n",
       "       [ 822.66094971]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 740.24932861]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i in range(0, len(final_results))]\n",
    "final_output = pd.DataFrame({'row_id': index})\n",
    "final_output['duration'] = final_results[:,0]\n",
    "final_output['duration'] = final_output['duration'].astype(int)\n",
    "final_results[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_output.to_csv(\"duration.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>105</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>108</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id  duration\n",
       "100     101       676\n",
       "101     102       666\n",
       "102     103       593\n",
       "103     104       675\n",
       "104     105       529\n",
       "105     106       610\n",
       "106     107       681\n",
       "107     108       595\n",
       "108     109       346\n",
       "109     110       581"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/mchirico/bcc376fb336b73f24b29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def assert_geohashes_in_set(df, reverse_dictionary):\n",
    "    for index, row in df.iterrows():\n",
    "        missing = set()\n",
    "        if not reverse_dictionary.get(row[\"start_geohash\"]):\n",
    "            missing.add(row[\"start_geohash\"])\n",
    "        if not reverse_dictionary.get(row[\"end_geohash\"]):\n",
    "            missing.add(row[\"end_geohash\"])\n",
    "        if len(missing) > 0:\n",
    "            raise Exception(\"end_geohash is not in global dataset: %s\"% len(missing))\n",
    "\n",
    "def transform_input():\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    df['start_geohash'] = df.apply(geohash_encode_start, axis=1)\n",
    "    print 'end geohash'\n",
    "    df['end_geohash'] = df.apply(geohash_encode_end, axis=1)    \n",
    "#     df[\"start_timestamp_obj\"] = df.apply(get_local_time, axis=1)\n",
    "#     df.to_csv(\"test_mod.csv\")\n",
    "#     df[\"start_of_day\"] = df.apply(get_start_of_the_day, axis=1)\n",
    "#     df[\"weekday\"] = df.apply(get_weekday, axis=1)\n",
    "#     df[\"hour\"] = df.apply(get_hour, axis=1)\n",
    "#     df[\"minute\"] = df.apply(get_min, axis=1)\n",
    "    return df\n",
    "\n",
    "def handle_test_data():\n",
    "    df = pd.read_csv(\"test.csv\")\n",
    "    df = transform_input(df)\n",
    "    assert_geohashes_in_set(df, reverse_dictionary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "end_geohash is not in global dataset: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-ed8a8f1e3703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dftest = transform_input()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# assert_geohashes_in_set(dftest, reverse_dictionary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0massert_geohashes_in_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdftest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-261-4adbaa7bf1e5>\u001b[0m in \u001b[0;36massert_geohashes_in_set\u001b[0;34m(df, reverse_dictionary)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_geohash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"end_geohash is not in global dataset: %s\"\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: end_geohash is not in global dataset: 1"
     ]
    }
   ],
   "source": [
    "# dftest = transform_input()\n",
    "# assert_geohashes_in_set(dftest, reverse_dictionary)\n",
    "assert_geohashes_in_set(dftest, reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def input_placeholders():\n",
    "    with tf.name_scope('input_layer'):\n",
    "        features = tf.placeholder(tf.float32, shape=[None, INPUT_FEATURES])\n",
    "        train_labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    return features, train_labels\n",
    "def inference(features, hidden1_units, hidden2_units, hidden3_units):\n",
    "    \"\"\" Building the tensorflow model \"\"\"\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights_1 = tf.Variable(\n",
    "            tf.truncated_normal([INPUT_FEATURES, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(INPUT_FEATURES))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(features, weights_1) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights_2 = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights_2) + biases)\n",
    "    # Hidden 3\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights_3 = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, hidden3_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden3_units]),\n",
    "                             name='biases')\n",
    "        hidden3 = tf.nn.relu(tf.matmul(hidden2, weights_3) + biases)\n",
    "        # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden3_units, 1],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden3_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([1]),\n",
    "                             name='biases')\n",
    "        output_linear = tf.matmul(hidden3, weights) + biases\n",
    "    return output_linear, (weights, weights_1, weights_2)\n",
    "\n",
    "def get_loss(output_linear, labels, n_samples):\n",
    "#     cost = tf.reduce_mean(tf.pow(output_linear-labels, 2))/(2*n_samples)\n",
    "    cost = tf.reduce_sum(tf.square(output_linear-labels))/(400000)\n",
    "#     reduction = tf.square(tf.subtract(output_linear, labels))\n",
    "#     red = tf.reduce_mean(reduction)\n",
    "    return cost\n",
    "\n",
    "def training(loss, starter_learning_rate, decay_steps):\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               decay_steps, 0.96, staircase=False)   \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def get_evaluation(output_linear, labels):\n",
    "\n",
    "    diff = tf.abs(tf.subtract(output_linear, labels))\n",
    "    return tf.reduce_mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data = df[:training_size]\n",
    "test_data = df[testing_size + 1:]\n",
    "input_batch = 20000\n",
    "batches = get_all_batches(input_batch, training_data[:500000])\n",
    "batch_index = 0\n",
    "batch_list_len = len(batches)\n",
    "validation_data, validation_labels = get_test_batch(10000, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data_size = vocabulary_size\n",
    "# training_size = vocabulary_size - vocabulary_size//10\n",
    "# testing_size = data_size - training_size\n",
    "final_results = None\n",
    "batch_index = 0\n",
    "num_steps = 5000000\n",
    "starter_learning_rate = 0.0001\n",
    "decay_steps = 100000\n",
    "beta = 0.001\n",
    "def start_training(num_steps):\n",
    "#     sess = tf.InteractiveSession()\n",
    "#     tf.global_variables_initializer().run()\n",
    "    with tf.Graph().as_default():\n",
    "        features, labels = input_placeholders()\n",
    "        linear_inference, (w, w1, w2) = inference(features, 500, 500, 500)\n",
    "        loss = get_loss(linear_inference, labels, training_size)\n",
    "        reg_loss = loss + beta*tf.nn.l2_loss(w1) + beta*tf.nn.l2_loss(w2)\n",
    "        train_op = training(reg_loss, starter_learning_rate, decay_steps)\n",
    "        evaluation = get_evaluation(linear_inference, labels)\n",
    "#         summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "#         saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "#         summary_writer = tf.summary.FileWriter(\"/tmp/rides/\", sess.graph)\n",
    "        sess.run(init)\n",
    "        avg_loss = 0\n",
    "        for step in xrange(num_steps):\n",
    "            start_time = time.time()\n",
    "            batch_i = step % batch_list_len\n",
    "#             batch_data, label_data = batches[batch_i]\n",
    "            batch_data, label_data = get_batch(10000, training_data)\n",
    "            feed_dict = {features:batch_data, labels:label_data}\n",
    "            _, loss_value = sess.run([train_op, reg_loss],\n",
    "                               feed_dict=feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            avg_loss += loss_value\n",
    "\n",
    "          # Write the summaries and print an overview fairly often.\n",
    "            if step % 2000 == 0:\n",
    "                # Print status to stdout.\n",
    "                print('Step %d: avg loss = %.5f (%.3f sec)' % (step, avg_loss/2000, duration))\n",
    "                avg_loss = 0\n",
    "                # Update the events file.\n",
    "#                 summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "#                 summary_writer.add_summary(summary_str, step)\n",
    "#                 summary_writer.flush()\n",
    "            if step % 10000 == 0:\n",
    "#                 test_batch_data, test_label_data = get_test_batch(5000, training_data)\n",
    "                feed_dict = {features:validation_data, labels:validation_labels}\n",
    "                results = sess.run(evaluation, feed_dict=feed_dict)\n",
    "                print 'Mean difference in testing data : %s' % results\n",
    "#         batch_data = get_full_test_data()\n",
    "        feed_dict = {features:full_test_data}\n",
    "        global final_results\n",
    "        final_results = sess.run(linear_inference, feed_dict=feed_dict)\n",
    "start_training(num_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
